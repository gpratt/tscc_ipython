{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gscripts.general import parsers\n",
    "import pandas as pd\n",
    "import os\n",
    "from IPython.core.display import HTML\n",
    "import pybedtools\n",
    "from clipper.src import CLIP_analysis\n",
    "\n",
    "import numpy as np\n",
    "import functools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def eplouge(name,count):\n",
    "    return \"\"\"#!/bin/bash\n",
    "#PBS -N {0}\n",
    "#PBS -l nodes=1:ppn=1\n",
    "#PBS -o {0}.out\n",
    "#PBS -e {0}.err\n",
    "#PBS -V\n",
    "#PBS -q home-yeo\n",
    "#PBS -W group_list=yeo-group\n",
    "#PBS -t 1-{1}\n",
    "#PBS -l walltime=8:00:00\n",
    "cd /home/gpratt/projects/encode/analysis/peak_reanalysis_v12/\n",
    "echo \"hello, starting\"\n",
    "\"\"\".format(name, count)\n",
    "\n",
    "prolouge = \"eval ${cmd[$PBS_ARRAYID]}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def format_clip_analysis(bed_file, bam_file):\n",
    "    path, ext = os.path.splitext(os.path.basename(bed_file))\n",
    "    metrics = path + \".metrics\"\n",
    "    return \"clip_analysis --clusters {} -s hg19 --bam {} --AS_Structure /projects/ps-yeolab/genomes/hg19/hg19data4 --genome_location /projects/ps-yeolab/genomes/hg19/chromosomes/all.fa --phastcons_location /projects/ps-yeolab/genomes/hg19/hg19_phastcons.bw --nrand 3 --runPhast --metrics {} --gff_db /projects/ps-yeolab/genomes/hg19/gencode.v17.annotation.gtf.db\".format(bed_file, bam_file, metrics)\n",
    "\n",
    "def l2fc_filter(peak_file, l2fc= 3, p_val= 2):\n",
    "    bedtool = pybedtools.BedTool(peak_file)\n",
    "\n",
    "    bedtool = bedtool.filter(lambda interval: (float(interval.score) > l2fc) and (float(interval.name) > p_val))\n",
    "    bedtool = bedtool.each(move_name)\n",
    "    bedtool = bedtool.saveas(os.path.join(\"/home/gpratt/projects/encode/analysis/encode_v9_secondary_processing\", \n",
    "                                         os.path.basename(peak_file)) + \".filtered.bed\")\n",
    "    return bedtool.fn\n",
    "\n",
    "def move_name(interval):\n",
    "    interval.name = interval[7]\n",
    "    return pybedtools.create_interval_from_list(interval[:6])\n",
    "    return interval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Run l2f_1 analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = pd.read_table(\"/home/elvannostrand/data/clip/CLIPseq_analysis/scripts/l2f_1_filelist\", \n",
    "              header=False, names=['encode_id', \n",
    "                                   'rbp', \n",
    "                                   'cell_type', \n",
    "                                   'bam_file_1', \n",
    "                                   'bam_file_2', \n",
    "                                   'input_file', \n",
    "                                   'peaks_file_1', \n",
    "                                   'peaks_file_2'])\n",
    "\n",
    "\n",
    "#tidy the data up\n",
    "result = []\n",
    "for index, line in df.iterrows():\n",
    "    result.append({'encode_id': line.encode_id,\n",
    "                   'rep': 1,\n",
    "                   'rbp': line.rbp, \n",
    "                   'cell_type': line.cell_type, \n",
    "                   'bam_file': line.bam_file_1, \n",
    "                   'input_file': line.input_file, \n",
    "                   'peaks_file': line.peaks_file_1, })\n",
    "    result.append({'encode_id': line.encode_id,\n",
    "                       'rep': 2,\n",
    "                       'rbp': line.rbp, \n",
    "                       'cell_type': line.cell_type, \n",
    "                       'bam_file': line.bam_file_2, \n",
    "                       'input_file': line.input_file, \n",
    "                       'peaks_file': line.peaks_file_2, })\n",
    "\n",
    "df = pd.DataFrame(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "total = 0 \n",
    "result = []\n",
    "for x, row in df.iterrows():\n",
    "    total += 1\n",
    "    result.append('cmd[{}]=\"{}\"'.format(total, format_clip_analysis(row.peaks_file, row.bam_file)))\n",
    "    \n",
    "with open(os.path.join(\"/home/gpratt/projects/encode/scripts\", \"encode_l2f_1_analysis.sh\"), 'w') as out_file:\n",
    "    out_file.write(eplouge(\"encode_l2f_4\", total))\n",
    "    for line in result:\n",
    "        out_file.write(line + \"\\n\\n\")\n",
    "    out_file.write(prolouge + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Run l2f_4 analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = pd.read_table(\"/home/gpratt/projects/encode/scripts/l2f_4_filelist\",\n",
    "    #\"/home/elvannostrand/data/clip/CLIPseq_analysis/scripts/l2f_4_filelist\", \n",
    "              header=False, names=['encode_id', \n",
    "                                   'rbp', \n",
    "                                   'cell_type', \n",
    "                                   'bam_file_1', \n",
    "                                   'bam_file_2', \n",
    "                                   'input_file', \n",
    "                                   'peaks_file_1', \n",
    "                                   'peaks_file_2'])\n",
    "\n",
    "\n",
    "#tidy the data up\n",
    "result = []\n",
    "for index, line in df.iterrows():\n",
    "    result.append({'encode_id': line.encode_id,\n",
    "                   'rep': 1,\n",
    "                   'rbp': line.rbp, \n",
    "                   'cell_type': line.cell_type, \n",
    "                   'bam_file': line.bam_file_1, \n",
    "                   'input_file': line.input_file, \n",
    "                   'peaks_file': line.peaks_file_1, })\n",
    "    result.append({'encode_id': line.encode_id,\n",
    "                       'rep': 2,\n",
    "                       'rbp': line.rbp, \n",
    "                       'cell_type': line.cell_type, \n",
    "                       'bam_file': line.bam_file_2, \n",
    "                       'input_file': line.input_file, \n",
    "                       'peaks_file': line.peaks_file_2, })\n",
    "\n",
    "df = pd.DataFrame(result)\n",
    "\n",
    "total = 0 \n",
    "result = []\n",
    "for x, row in df.iterrows():\n",
    "    total += 1\n",
    "    result.append('cmd[{}]=\"{}\"'.format(total, format_clip_analysis(row.peaks_file, row.bam_file)))\n",
    "    \n",
    "with open(os.path.join(\"/home/gpratt/projects/encode/scripts\", \"encode_l2f_4_analysis.sh\"), 'w') as out_file:\n",
    "    out_file.write(eplouge(\"encode_l2f_4\", total))\n",
    "    for line in result:\n",
    "        out_file.write(line + \"\\n\\n\")\n",
    "    out_file.write(prolouge + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Run Sebas analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SEB1\tG3BP\tX\t/home/gpratt/projects/encode/analysis/encode_v9/G3BP_S_rep1_G3BP1.merged.r2.bam\t/home/gpratt/projects/encode/analysis/encode_v9/G3BP_S_rep2_G3BP1.merged.r2.bam\t/home/gpratt/projects/encode/analysis/encode_v9/G3BP-S-input_S1_L001_R1.unassigned.adapterTrim.round2.rmRep.rmDup.sorted.r2.bam\t/home/elvannostrand/data/clip/CLIPseq_analysis/SM_v9_20151211/SEB1_01.basedon_SEB1_01.peaks.l2inputnormnew.bed\t/home/elvannostrand/data/clip/CLIPseq_analysis/SM_v9_20151211/SEB1_02.basedon_SEB1_02.peaks.l2inputnormnew.bed\r\n",
      "SEB2\tG3BP\tX\t/home/gpratt/projects/encode/analysis/encode_v9/G3BP_US_rep1_G3BP1.merged.r2.bam\t/home/gpratt/projects/encode/analysis/encode_v9/G3BP_US_rep2_G3BP1.merged.r2.bam\t/home/gpratt/projects/encode/analysis/encode_v9/G3BP-US-input_S3_L001_R1.unassigned.adapterTrim.round2.rmRep.rmDup.sorted.r2.bam\t/home/elvannostrand/data/clip/CLIPseq_analysis/SM_v9_20151211/SEB2_01.basedon_SEB2_01.peaks.l2inputnormnew.bed\t/home/elvannostrand/data/clip/CLIPseq_analysis/SM_v9_20151211/SEB2_02.basedon_SEB2_02.peaks.l2inputnormnew.bed\r\n",
      "SEB3\tTIA1\tX\t/home/gpratt/projects/encode/analysis/encode_v9/TIA_S_rep1_TIA1.merged.r2.bam\t/home/gpratt/projects/encode/analysis/encode_v9/TIA_S_rep2_TIA1.merged.r2.bam\t/home/gpratt/projects/encode/analysis/encode_v9/TIA1-S-input_S5_L001_R1.unassigned.adapterTrim.round2.rmRep.rmDup.sorted.r2.bam\t/home/elvannostrand/data/clip/CLIPseq_analysis/SM_v9_20151211/SEB3_01.basedon_SEB3_01.peaks.l2inputnormnew.bed\t/home/elvannostrand/data/clip/CLIPseq_analysis/SM_v9_20151211/SEB3_02.basedon_SEB3_02.peaks.l2inputnormnew.bed\r\n",
      "SEB4\tTIA1\tX\t/home/gpratt/projects/encode/analysis/encode_v9/TIA1_US_rep1_TIA1.merged.r2.bam\t/home/gpratt/projects/encode/analysis/encode_v9/TIA1_US_rep2_TIA1.merged.r2.bam\t/home/gpratt/projects/encode/analysis/encode_v9/TIA1-US-input_S7_L001_R1.unassigned.adapterTrim.round2.rmRep.rmDup.sorted.r2.bam\t/home/elvannostrand/data/clip/CLIPseq_analysis/SM_v9_20151211/SEB4_01.basedon_SEB4_01.peaks.l2inputnormnew.bed\t/home/elvannostrand/data/clip/CLIPseq_analysis/SM_v9_20151211/SEB4_02.basedon_SEB4_02.peaks.l2inputnormnew.bed\r\n"
     ]
    }
   ],
   "source": [
    "!head /home/elvannostrand/data/clip/CLIPseq_analysis/SM_v9_20151211/encode_v9_filelist_SM_20151211.txt.l2foldenr_files.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = pd.read_table(\"/home/elvannostrand/data/clip/CLIPseq_analysis/SM_v9_20151211/encode_v9_filelist_SM_20151211.txt.l2foldenr_files.txt\",\n",
    "              header=None, names=['encode_id', \n",
    "                                   'rbp', \n",
    "                                   'cell_type', \n",
    "                                   'bam_file_1', \n",
    "                                   'bam_file_2', \n",
    "                                   'input_file', \n",
    "                                   'peaks_file_1', \n",
    "                                   'peaks_file_2'])\n",
    "\n",
    "\n",
    "#tidy the data up\n",
    "result = []\n",
    "for index, line in df.iterrows():\n",
    "    result.append({'encode_id': line.encode_id,\n",
    "                   'rep': 1,\n",
    "                   'rbp': line.rbp, \n",
    "                   'cell_type': line.cell_type, \n",
    "                   'bam_file': line.bam_file_1, \n",
    "                   'input_file': line.input_file, \n",
    "                   'peaks_file': line.peaks_file_1, })\n",
    "    result.append({'encode_id': line.encode_id,\n",
    "                       'rep': 2,\n",
    "                       'rbp': line.rbp, \n",
    "                       'cell_type': line.cell_type, \n",
    "                       'bam_file': line.bam_file_2, \n",
    "                       'input_file': line.input_file, \n",
    "                       'peaks_file': line.peaks_file_2, })\n",
    "\n",
    "df = pd.DataFrame(result)\n",
    "df['annotated_peaks'] = df.peaks_file.apply(lambda x: x + \".compressed.bed.annotated\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df['filtered_peaks'] = df.annotated_peaks.apply(l2fc_filter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "total = 0 \n",
    "result = []\n",
    "for x, row in df.iterrows():\n",
    "    total += 1\n",
    "    result.append('cmd[{}]=\"{}\"'.format(total, format_clip_analysis(row.filtered_peaks, row.bam_file)))\n",
    "    \n",
    "with open(os.path.join(\"/home/gpratt/projects/encode/scripts\", \"sebas_analysis.sh\"), 'w') as out_file:\n",
    "    out_file.write(eplouge(\"sebas\", total))\n",
    "    for line in result:\n",
    "        out_file.write(line + \"\\n\\n\")\n",
    "    out_file.write(prolouge + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Run Analysis for Eric\n",
    "This may not work anymore, I think I fixed clip analysis to be at the head again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ev42 = format_clip_analysis(\"/home/elvannostrand/data/clip/CLIPseq_analysis/ALLCLIP_v12_20160112/EV42_01.basedon_EV42_01.peaks.l2inputnormnew.bed.compressed.bed.l10p_5_l2fc_3.bed\", \n",
    "                     \"/projects/ps-yeolab2/encode/analysis/encode_v12/293XT_CLIP_RBFOX2_1120_RBFOX2.merged.r2.bam\")\n",
    "\n",
    "ev43 = format_clip_analysis(\"/home/elvannostrand/data/clip/CLIPseq_analysis/ALLCLIP_v12_20160112/EV43_01.basedon_EV43_01.peaks.l2inputnormnew.bed.compressed.bed.l10p_5_l2fc_3.bed\",\n",
    "                     \"/projects/ps-yeolab2/encode/analysis/encode_v12/293XT_CLIP_RBFOX2_0204_RBFOX2.merged.r2.bam\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#Run Input Normalized Filter for all things"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = pd.read_table(\"/home/elvannostrand/data/clip/CLIPseq_analysis/KB_v12_20160201/encode_v12_filelist_KB_20160201_nosingle.txt.l2foldenr_files.txt\", \n",
    "              header=None,\n",
    "             names=['encode_id', \n",
    "                       'rbp', \n",
    "                       'cell_type', \n",
    "                       'bam_file_1', \n",
    "                       'bam_file_2', \n",
    "                       'input_file', \n",
    "                       'peaks_file_1', \n",
    "                       'peaks_file_2'])\n",
    "\n",
    "#tidy the data up\n",
    "result = []\n",
    "for index, line in df.iterrows():\n",
    "    result.append({'encode_id': line.encode_id,\n",
    "                   'rep': 1,\n",
    "                   'rbp': line.rbp, \n",
    "                   'cell_type': line.cell_type, \n",
    "                   'bam_file': line.bam_file_1, \n",
    "                   'input_file': line.input_file, \n",
    "                   'peaks_file': line.peaks_file_1, })\n",
    "    result.append({'encode_id': line.encode_id,\n",
    "                       'rep': 2,\n",
    "                       'rbp': line.rbp, \n",
    "                       'cell_type': line.cell_type, \n",
    "                       'bam_file': line.bam_file_2, \n",
    "                       'input_file': line.input_file, \n",
    "                       'peaks_file': line.peaks_file_2, })\n",
    "\n",
    "df = pd.DataFrame(result)\n",
    "df['annotated_peaks'] = df.peaks_file.apply(lambda x: x + \".compressed.bed.annotated\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df['filtered_peaks'] = df.annotated_peaks.apply(l2fc_filter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KB13-14\n",
      "38 105 12\n",
      "KB17-18\n",
      "365 408 117\n",
      "KB2-3\n",
      "50 134 25\n",
      "KB20-21\n",
      "1526 1276 320\n",
      "KB4-16\n",
      "164 283 19\n",
      "KB6-7\n",
      "1560 2476 568\n",
      "KB9-10\n",
      "1277 651 120\n"
     ]
    }
   ],
   "source": [
    "total = 0 \n",
    "result = []\n",
    "for x, row in df.iterrows():\n",
    "    total += 1\n",
    "    result.append('cmd[{}]=\"{}\"'.format(total, format_clip_analysis(row.filtered_peaks, row.bam_file)))\n",
    "    \n",
    "for name, row in df.groupby(\"encode_id\"):\n",
    "    print name\n",
    "\n",
    "    rep1, rep2 = row.filtered_peaks\n",
    "\n",
    "    rep1 = pybedtools.BedTool(rep1)\n",
    "    rep2 = pybedtools.BedTool(rep2)\n",
    "\n",
    "    merged_peaks = rep1.intersect(rep2, s=True, u=True).saveas(os.path.join(\"/home/gpratt/projects/encode/analysis/encode_v9_secondary_processing\", \n",
    "                                                                            name + \".merged.compressed.bed.annotated.filtered.bed\"))\n",
    "    print len(rep1), len(rep2), len(merged_peaks)\n",
    "    result.append('cmd[{}]=\"{}\"'.format(total, format_clip_analysis(merged_peaks.fn, row.bam_file.values[0])))\n",
    "    \n",
    "with open(os.path.join(\"/home/gpratt/projects/encode/scripts\", \"kb_analysis.sh\"), 'w') as out_file:\n",
    "    out_file.write(eplouge(\"sebas\", total))\n",
    "    for line in result:\n",
    "        out_file.write(line + \"\\n\\n\")\n",
    "        \n",
    "\n",
    "    out_file.write(prolouge + \"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
