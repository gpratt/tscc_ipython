{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "from collections import defaultdict\n",
    "import functools\n",
    "import os\n",
    "import urllib\n",
    "\n",
    "from IPython.core.display import HTML\n",
    "from Bio import SeqIO\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pybedtools\n",
    "import pysam\n",
    "import seaborn as sns\n",
    "import matplotlib\n",
    "from matplotlib import gridspec\n",
    "import scipy\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "tqdm.pandas(\"Progress: \")\n",
    "\n",
    "from gscripts.general import dataviz\n",
    "from gscripts.encode import encode_helpers\n",
    "from gscripts import qtools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "img_dir = \"/home/gpratt/Dropbox/encode_integration/qc_work/\"\n",
    "\n",
    "sns.set_style(\"ticks\")\n",
    "xfmt = matplotlib.ticker.FuncFormatter(lambda x, p: format(float(x) / 1000000, ','))\n",
    "xfmt_int = matplotlib.ticker.FuncFormatter(lambda x, p: format(int(float(x) / 1000000), ','))\n",
    "\n",
    "RESET = False\n",
    "\n",
    "#reads = \"family_reads\"\n",
    "reads = \"reads\"\n",
    "\n",
    "unique = \"unique\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "legend = None\n",
    "\n",
    "OUTSIDE_LEGEND_SAVEFIG_KWS = dict(bbox_extra_artists=(legend,),\n",
    "                                  bbox_inches='tight')\n",
    "from matplotlib import rc\n",
    "\n",
    "mpl.rcParams['svg.fonttype'] = 'none'\n",
    "\n",
    "rc('text', usetex=False) \n",
    "rc('font',**{'family':'sans-serif','sans-serif':['Helvetica']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "out_dir = \"/home/gpratt/projects/encode/analysis/peak_reanalysis_v14/\"\n",
    "downsample_path = \"/home/gpratt/projects/idr/analysis/downsample_v2/\"\n",
    "ew_input_norm_dir = \"/home/elvannostrand/data/clip/CLIPseq_analysis/EW_v9_2015210\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "merged_data = encode_helpers.get_merged_data()\n",
    "\n",
    "#For Peak Analysis we only want to analyze datasets that have been submitted\n",
    "merged_data = merged_data[merged_data.submitted]\n",
    "merged_data = merged_data[['CLIP', 'INPUT', 'input_norm']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "181.0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(merged_data) / 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Downsampling file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Ignoring this for now, don't need it\n",
    "#Add Emily and Erics data to the dataframe for processing\n",
    "# rep1 = \"/projects/ps-yeolab3/encode/analysis/encode_master/EW2_CLIP_U2AF1_V5.merged.r2.bam\"\n",
    "# rep2 = \"/projects/ps-yeolab3/encode/analysis/encode_master/EW3_CLIP_U2AF1_V5.merged.r2.bam\"\n",
    "# input_fn = \"/projects/ps-yeolab3/encode/analysis/encode_master/EW1_INPUT_ATTACTCG-TATAGCCT_L007_R1.unassigned.adapterTrim.round2.rmRep.rmDup.sorted.r2.bam\"\n",
    "# out_list = [rep1, rep2, input_fn, \"hg19\", \"EW\"]\n",
    "# out_file.write(\"\\t\".join(out_list) + \"\\n\")\n",
    "\n",
    "# submitted_datasets.loc[181] = [\"EW\", \"U2AF1_V5\", \"HepG2\", rep1, rep2, input_fn]\n",
    "# submitted_datasets.loc[182] = [\"293XT_CLIP_RBFOX2_0204\", \"RBFOX2\", \"293T\", \n",
    "#                            \"/projects/ps-yeolab3/encode/analysis/encode_v12/293XT_CLIP_RBFOX2_0204_RBFOX2.merged.r2.bam\", \n",
    "#                            \"/projects/ps-yeolab3/encode/analysis/encode_v12/293XT_CLIP_RBFOX2_0204_RBFOX2.merged.r2.bam\",\n",
    "#                            \"/projects/ps-yeolab3/encode/analysis/encode_v12/293XT_Input_0204_ATTCAGAA-TAATCTTA_R1.unassigned.adapterTrim.round2.rmRep.rmDup.sorted.r2.bam\"]\n",
    "# submitted_datasets.loc[183] = [\"293XT_CLIP_RBFOX2_1120\", \"RBFOX2\", \"293T\", \n",
    "#                            \"/projects/ps-yeolab3/encode/analysis/encode_v12/293XT_CLIP_RBFOX2_1120_RBFOX2.merged.r2.bam\", \n",
    "#                            \"/projects/ps-yeolab3/encode/analysis/encode_v12/293XT_CLIP_RBFOX2_1120_RBFOX2.merged.r2.bam\", \n",
    "#                            \"/projects/ps-yeolab3/encode/analysis/encode_v12/293XT_Input_1120_GAATTCGT-GGCTCTGA_R1.unassigned.adapterTrim.round2.rmRep.rmDup.sorted.r2.bam\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open(\"/home/gpratt/projects/idr/scripts/downsample_v2.txt\", 'w') as out_file:\n",
    "#     out_list = [rep1, rep2, input_fn, \"hg19\", \"EW\"]\n",
    "#     out_file.write(\"\\t\".join(out_list) + \"\\n\")\n",
    "\n",
    "    for name, row in merged_data.unstack().iterrows():\n",
    "        uID = name[0]\n",
    "        out_list = [row[('CLIP', \"rep1\")], row[('CLIP', \"rep2\")], row[('INPUT', \"rep1\")], \"hg19\", uID]\n",
    "        out_file.write(\"\\t\".join(out_list) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# #Converts reps into dataframe, like they should be\n",
    "# submitted_datasets = submitted_datasets.set_index([\"uID\", \"RBP\", \"Cell line\", \"INPUT\"])\n",
    "# submitted_datasets.columns = pd.MultiIndex.from_tuples([item.split(\"_\") for item in submitted_datasets.columns])\n",
    "# submitted_datasets = submitted_datasets.stack()\n",
    "# submitted_datasets['INPUT'] = submitted_datasets.index.get_level_values(level=\"INPUT\")\n",
    "# submitted_datasets.index = submitted_datasets.index.droplevel(\"INPUT\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Format to replicate for downsampling\n",
    "\n",
    "ip 1\\t ip 2 \\t input \\t species \\t id\n",
    "\n",
    "\n",
    "/projects/ps-yeolab3/encode/analysis/encode_v12/213_02_RBM34.merged.r2.bam      /projects/ps-yeolab3/encode/analysis/encode_v12/231_01_RBM34.merged.r2.bam      /projects/ps-yeolab3/encode/analysi\n",
    "s/encode_v12/213_INPUT_ATTCAGAA-ATAGAGGC_L002_R1.unassigned.adapterTrim.round2.rmRep.rmDup.sorted.r2.bam     hg19    213  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot Peak Level downsampling and correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "make_and_filter_clipper_moderate = functools.partial(encode_helpers.make_and_filter_clipper, l2fc=3, pval=3, reset=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Get all downsampled datasets and also put them into a tidy dataframe\n",
    "merged_data = merged_data.set_index(\"INPUT\", append=True)\n",
    "\n",
    "merged_data.columns = pd.MultiIndex.from_tuples([(\"CLIP\", 1.0), (\"input_norm\", 1.0)], names=[\"input_norm\", \"fraction\"])\n",
    "\n",
    "for percent in range(1,10):\n",
    "    percent_downsample = \"0{}\".format(percent)\n",
    "    merged_data[('CLIP', percent / 10.)] = merged_data[(\"CLIP\", 1.0)].apply(lambda x: downsample_path + os.path.basename(x).replace(\".bam\", \".{}.bam\".format(percent_downsample)))\n",
    "    merged_data[('input_norm', percent / 10.)] = merged_data[(\"CLIP\", 1.0)].apply(lambda x: downsample_path + os.path.basename(x).replace(\".bam\", \".{}.peaks.norm.compressed.bed\".format(percent_downsample)))\n",
    "\n",
    "merged_data = merged_data.stack()\n",
    "merged_data = merged_data.reset_index()\n",
    "merged_data = merged_data.set_index([u'uID',u'RBP', u'Cell line', \"rep\", u'fraction'])\n",
    "merged_data = merged_data.sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'uID'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-7d1551618c2b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mect_and_usable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mect_and_usable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mect_and_usable\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'rep'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mect_and_usable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbio_rep\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"rep1\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"rep2\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0minitial_counts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmerge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfoo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mect_and_usable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mleft_on\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'uID'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'RBP'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rep'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'fraction'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mright_on\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'RBP_ID'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'RBP'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rep'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'fraction'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0minitial_counts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minitial_counts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"uID\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'RBP'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Cell line'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rep'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'fraction'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/gpratt/anaconda/lib/python2.7/site-packages/pandas/tools/merge.pyc\u001b[0m in \u001b[0;36mmerge\u001b[0;34m(left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator)\u001b[0m\n\u001b[1;32m     32\u001b[0m                          \u001b[0mright_on\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mright_on\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mleft_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mleft_index\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m                          \u001b[0mright_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mright_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msort\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msort\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msuffixes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msuffixes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m                          copy=copy, indicator=indicator)\n\u001b[0m\u001b[1;32m     35\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__debug__\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/gpratt/anaconda/lib/python2.7/site-packages/pandas/tools/merge.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, left, right, how, on, left_on, right_on, axis, left_index, right_index, sort, suffixes, copy, indicator)\u001b[0m\n\u001b[1;32m    197\u001b[0m         (self.left_join_keys,\n\u001b[1;32m    198\u001b[0m          \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mright_join_keys\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m          self.join_names) = self._get_merge_keys()\n\u001b[0m\u001b[1;32m    200\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/gpratt/anaconda/lib/python2.7/site-packages/pandas/tools/merge.pyc\u001b[0m in \u001b[0;36m_get_merge_keys\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    430\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    431\u001b[0m                         \u001b[0mright_keys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 432\u001b[0;31m                     \u001b[0mleft_keys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mleft\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    433\u001b[0m                     \u001b[0mjoin_names\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0m_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mleft_on\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/gpratt/anaconda/lib/python2.7/site-packages/pandas/core/frame.pyc\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1990\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1991\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1992\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_column\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1993\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1994\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_getitem_column\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/gpratt/anaconda/lib/python2.7/site-packages/pandas/core/frame.pyc\u001b[0m in \u001b[0;36m_getitem_column\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1997\u001b[0m         \u001b[0;31m# get column\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1998\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_unique\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1999\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_item_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2000\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2001\u001b[0m         \u001b[0;31m# duplicate columns & possible reduce dimensionality\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/gpratt/anaconda/lib/python2.7/site-packages/pandas/core/generic.pyc\u001b[0m in \u001b[0;36m_get_item_cache\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m   1343\u001b[0m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1344\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mres\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1345\u001b[0;31m             \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1346\u001b[0m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_box_item_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1347\u001b[0m             \u001b[0mcache\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/gpratt/anaconda/lib/python2.7/site-packages/pandas/core/internals.pyc\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, item, fastpath)\u001b[0m\n\u001b[1;32m   3223\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3224\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misnull\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3225\u001b[0;31m                 \u001b[0mloc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3226\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3227\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0misnull\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/gpratt/anaconda/lib/python2.7/site-packages/pandas/indexes/base.pyc\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   1876\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1877\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1878\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_cast_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1879\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1880\u001b[0m         \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtolerance\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtolerance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/index.pyx\u001b[0m in \u001b[0;36mpandas.index.IndexEngine.get_loc (pandas/index.c:4027)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/index.pyx\u001b[0m in \u001b[0;36mpandas.index.IndexEngine.get_loc (pandas/index.c:3891)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/hashtable.pyx\u001b[0m in \u001b[0;36mpandas.hashtable.PyObjectHashTable.get_item (pandas/hashtable.c:12408)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/hashtable.pyx\u001b[0m in \u001b[0;36mpandas.hashtable.PyObjectHashTable.get_item (pandas/hashtable.c:12359)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'uID'"
     ]
    }
   ],
   "source": [
    "#ON RESET FIX THIS TO BE INLINE WITH EVERYTHING ELSE, UNTIL THEN FUCK IT\n",
    "foo = merged_data.copy()\n",
    "ect_and_usable = pd.read_csv(\"ect_estimate_usable_reads.csv\", index_col=[0,1,2,3])\n",
    "ect_and_usable = ect_and_usable[['uncollapsed', 'unique', 'eCT', 'fraction_usable', 'adjusted_ect_concentration']]\n",
    "ect_and_usable = ect_and_usable.reset_index()\n",
    "ect_and_usable['rep'] = ect_and_usable.bio_rep.apply(lambda x: \"rep1\" if x == 1 else \"rep2\")\n",
    "initial_counts = pd.merge(foo, ect_and_usable, left_on=['uID', 'RBP', 'rep', 'fraction'], right_on=['RBP_ID', 'RBP', 'rep', 'fraction'])\n",
    "initial_counts = initial_counts.set_index([\"uID\", 'RBP', 'Cell line', 'rep', 'fraction'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "merged_data['filtered_moderate'] = merged_data['input_norm'].progress_apply(make_and_filter_clipper_moderate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def bedtools_from_row(row):\n",
    "    total_bedtool = total_bedtools.ix[row.name[:-1]].filtered_moderate\n",
    "    fractional_bedtool = row.filtered_moderate\n",
    "    \n",
    "    total_bedtool = pybedtools.BedTool(total_bedtool)\n",
    "    fractional_bedtool = pybedtools.BedTool(fractional_bedtool)\n",
    "    return fractional_bedtool, total_bedtool\n",
    "\n",
    "def fraction_bound(row):\n",
    "    fractional_bedtool, total_bedtool = bedtools_from_row(row)\n",
    "    return len(total_bedtool.intersect(fractional_bedtool, u=True, s=True, sorted=True)) / float(len(total_bedtool))\n",
    " \n",
    "def fraction_of_fraction_in_total(row):\n",
    "    fractional_bedtool, total_bedtool = bedtools_from_row(row)\n",
    "    return len(fractional_bedtool.intersect(total_bedtool, u=True, s=True, sorted=True)) / float(len(fractional_bedtool))\n",
    "\n",
    "def fraction_not_in_total(row):\n",
    "    fractional_bedtool, total_bedtool = bedtools_from_row(row)\n",
    "    return len(fractional_bedtool.intersect(total_bedtool, v=True, s=True, sorted=True)) / float(len(fractional_bedtool))\n",
    "\n",
    "def get_entropy_from_annotated(fn):\n",
    "    fn = os.path.basename(fn)\n",
    "    stripped_fn = \".\".join(fn.split(\".\")[:-3])\n",
    "    stripped_fn = os.path.join(out_dir, stripped_fn)\n",
    "    return stripped_fn + \".full.compressed2.bed.full.entropy.bed\"\n",
    "\n",
    "def when_peak_recovered_beds(df):\n",
    "\n",
    "    \"\"\"Returns a DF annotated with peaks recovered at each round of downsampling.  Also returns those peaks as list of bedtools.  \n",
    "    \n",
    "    row is fully_downsampled_data row\n",
    "    rep is a string either \"rep1 or \"rep2\"\n",
    "    \"\"\"\n",
    "        \n",
    "    #Setup empty objects to append to\n",
    "    peaks = {}\n",
    "    all_peaks = df.filtered_moderate \n",
    "    total_bed = pybedtools.BedTool(total_bedtools.ix[df.ix[0].name[:-1]].entropy_tools)\n",
    "    interval = total_bed[0]\n",
    "    interval.start = 0\n",
    "    interval.stop = 1\n",
    "    total_peaks_detected = pybedtools.BedTool([interval])\n",
    "    \n",
    "    #Assume sorted order of fractions\n",
    "    for name, fraction in all_peaks.iteritems():\n",
    "\n",
    "        fraction_bed = pybedtools.BedTool(fraction)\n",
    "        \n",
    "        #Get the peaks that exist total that also exist in this fraction\n",
    "        fraction_peaks_bed = total_bed.intersect(fraction_bed, u=True, s=True, sorted=True)\n",
    "        \n",
    "        #Make sure peaks haven't been detected in previous fractions\n",
    "        fraction_peaks_bed = fraction_peaks_bed.intersect(total_peaks_detected, v=True, s=True, sorted=True)\n",
    "            \n",
    "        fraction_peaks_bed = fraction_peaks_bed.saveas(\"{}.{}.peaks.bed\".format(total_bed.fn, os.path.basename(fraction)))\n",
    "        if len(fraction_peaks_bed) != 0:\n",
    "            total_peaks_detected = total_peaks_detected.cat(fraction_peaks_bed, postmerge=False).sort()\n",
    "\n",
    "        peaks[name] = fraction_peaks_bed.fn\n",
    "\n",
    "    return pd.Series(peaks)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate family mapped number reads as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Get all the counts\n",
    "counts_list = pd.read_csv(\"downsample_counts_full_v4.csv\", dtype={\"RBP_ID\": str, \"bio_rep\": str, \"tech_rep\": str})\n",
    "counts_list = counts_list.set_index([\"RBP_ID\", \"RBP\", \"bio_rep\", \"tech_rep\", \"fraction\"])\n",
    "counts_list = counts_list.xs(1.0, level=\"fraction\").groupby(level=['RBP_ID', \"RBP\", \"bio_rep\"]).sum()\n",
    "counts_list.index.rename([\"uID\", \"RBP\", \"bio_rep\"], inplace=True)\n",
    "counts_list['rep'] = [\"rep1\" if rep == \"01\" else \"rep2\" for rep in counts_list.index.get_level_values(level=\"bio_rep\")]\n",
    "counts_list.index = counts_list.index.droplevel(\"bio_rep\")\n",
    "counts_list = counts_list.set_index(\"rep\", append=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def family_map_read_num(read_num):\n",
    "    x = np.arange(.1, 1.1, .1)\n",
    "    series = pd.Series(x * read_num, index=x)\n",
    "    series.index.name = \"fraction\"\n",
    "    return series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "family_map_num = counts_list.unique.apply(family_map_read_num).stack()\n",
    "family_map_num.index.name = \"family_reads\"\n",
    "family_map_num.index = pd.MultiIndex.from_tuples([(x[0], x[1], x[2], str(x[3])) for x in family_map_num.index], \n",
    "                                                 names=family_map_num.index.names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "merged_data = merged_data.reset_index(['Cell line'])\n",
    "\n",
    "merged_data.index = pd.MultiIndex.from_tuples([(x[0], x[1], x[2], str(x[3])) for x in merged_data.index], \n",
    "                                                 names=merged_data.index.names)\n",
    "\n",
    "merged_data['family_reads'] = family_map_num\n",
    "\n",
    "merged_data.index = pd.MultiIndex.from_tuples([(x[0], x[1], x[2], float(x[3])) for x in merged_data.index], \n",
    "                                                 names=merged_data.index.names)\n",
    "\n",
    "merged_data = merged_data.set_index([\"Cell line\"], append=True)\n",
    "merged_data = merged_data.reorder_levels(['uID','RBP', 'Cell line','rep', \"fraction\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "total_bedtools = merged_data.xs(1.0, level=\"fraction\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "merged_data['reads'] = merged_data.CLIP.apply(encode_helpers.get_mapped_reads)\n",
    "merged_data['fraction_overlap'] = merged_data.apply(fraction_bound, axis=1)\n",
    "merged_data['fraction_not_in_total'] = merged_data.apply(fraction_not_in_total, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "annotated_bedtool_header = ['chrom', 'start', \"stop\", \"name\", \"score\", \"strand\", \"annotation\", \"gene_id\"]\n",
    "result = []\n",
    "for name, row in tqdm(list(total_bedtools.iterrows())):\n",
    "    full_file = get_entropy_from_annotated(row.input_norm)\n",
    "    out_file = os.path.splitext(full_file)[0] + \".significant.bed\"\n",
    "    \n",
    "    if os.path.exists(out_file):\n",
    "        result.append(out_file)\n",
    "        continue\n",
    "        \n",
    "    entropy = pd.read_table(full_file)\n",
    "    peaks = pybedtools.BedTool(row.filtered_moderate).to_dataframe(names=annotated_bedtool_header)\n",
    "    merged_peaks = pd.merge(peaks, entropy, \n",
    "                            left_on=['chrom', 'start', 'stop'], right_on=['chrom', 'start', 'stop'], how=\"left\")\n",
    "    merged_peaks = merged_peaks.sort_values([\"chrom\", \"start\", \"stop\"])\n",
    "    if len(merged_peaks[merged_peaks.entropy.isnull()]) != 0:\n",
    "        print row.input_norm\n",
    "        \n",
    "    merged_peaks.to_csv(out_file, sep=\"\\t\", header=False, index=False)\n",
    "    result.append(out_file)\n",
    "total_bedtools['entropy_tools'] = result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "total_datasets = len(merged_data.groupby(level=0))\n",
    "num_cols = 4\n",
    "num_rows = (total_datasets / 4) + 1\n",
    "count = 0\n",
    "\n",
    "with dataviz.Figure(os.path.join(img_dir, \"downsampling_peaks.svg\"), figsize=(4* num_cols, 4*num_rows)) as fig:\n",
    "    for uID, df in merged_data.groupby(level='uID'):\n",
    "        count += 1\n",
    "        ax = fig.add_subplot(num_rows, num_cols, count)\n",
    "\n",
    "        for rep, dataset_df in df.groupby(level='rep'):\n",
    "            rbp_name = dataset_df.index.get_level_values(level=\"RBP\")[0]\n",
    "            name = \"{} {}\".format(uID, rbp_name)\n",
    "            ax.plot(dataset_df[reads], dataset_df.fraction_overlap, \n",
    "                    label=name, \n",
    "                    alpha=.7, \n",
    "                    linewidth=3)\n",
    "            ax.xaxis.set_major_formatter(xfmt)\n",
    "            ax.set_xlabel(\"Reads (M)\")\n",
    "            ax.set_ylabel(\"Fraction Overlapped\")\n",
    "            ax.set_ylim(0,1)\n",
    "            sns.despine(ax=ax)\n",
    "        ax.legend(fontsize=18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "total_datasets = len(merged_data.groupby(level=0))\n",
    "num_cols = 4\n",
    "num_rows = (total_datasets / 4) + 1\n",
    "count = 0\n",
    "\n",
    "with dataviz.Figure(os.path.join(img_dir, \"downsampling_peaks.svg\"), figsize=(4* num_cols, 4*num_rows)) as fig:\n",
    "    for uID, df in merged_data.groupby(level='uID'):\n",
    "        count += 1\n",
    "        ax = fig.add_subplot(num_rows, num_cols, count)\n",
    "\n",
    "        for rep, dataset_df in df.groupby(level='rep'):\n",
    "            rbp_name = dataset_df.index.get_level_values(level=\"RBP\")[0]\n",
    "            name = \"{} {}\".format(uID, rbp_name)\n",
    "            ax.plot(dataset_df[reads], dataset_df.fraction_not_in_total, \n",
    "                    label=name, \n",
    "                    alpha=.7, \n",
    "                    linewidth=3)\n",
    "            ax.xaxis.set_major_formatter(xfmt)\n",
    "            ax.set_xlabel(\"Reads (M)\")\n",
    "            ax.set_ylabel(\"Fraction Overlapped\")\n",
    "            ax.set_ylim(0,1)\n",
    "            sns.despine(ax=ax)\n",
    "        ax.legend(fontsize=18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "foo = merged_data.xs(.9, level=\"fraction\")\n",
    "foo[foo.fraction_overlap < .80]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#rbps_to_plot = [\"204\", \"205\", \"249\", \"241\", \"223\"]\n",
    "rbps_to_plot = [\"204\", \"242\", \"289\"]\n",
    "\n",
    "rbps_to_plot_df = merged_data.loc[rbps_to_plot]\n",
    "rbps_to_plot_df = rbps_to_plot_df.xs(\"rep1\", level=\"rep\")\n",
    "\n",
    "total_datasets = len(rbps_to_plot_df.groupby(level=0))\n",
    "num_cols = 1\n",
    "num_rows = 1\n",
    "count = 0\n",
    "\n",
    "with dataviz.Figure(os.path.join(img_dir, \"downsample_figure_fraction_presentation.svg\"), figsize=(4* num_cols, 4*num_rows)) as fig:\n",
    "    ax = fig.add_subplot(num_rows, num_cols, 1)\n",
    "    for uID, df in rbps_to_plot_df.groupby(level=0):\n",
    "        rbp_name = df.index.get_level_values(level=\"RBP\")[0]\n",
    "\n",
    "        name = \"{}\".format(rbp_name)\n",
    "        ax.plot(df.index.get_level_values(\"fraction\"), df.fraction_overlap, \n",
    "                label=name, \n",
    "                alpha=.7, \n",
    "                linewidth=5)\n",
    "        ax.set_xlabel(\"Fraction Downsampled\", fontsize=20)\n",
    "        ax.set_ylabel(\"Fraction Peaks\", fontsize=20)\n",
    "        ax.set_ylim(0,1)\n",
    "        ax.set_title(\"Fraction of\\nTotal Peaks Detected\", fontsize=20)\n",
    "        sns.despine(ax=ax)\n",
    "        ax.set_xticks(np.arange(0, 1.1, .2))\n",
    "        [tick.set_fontsize(16) for tick in ax.get_xticklabels()]\n",
    "        [tick.set_fontsize(16) for tick in ax.get_yticklabels()]\n",
    "        #ax.axhline(.90)\n",
    "        ax.axhline(.90, linestyle=\"--\", linewidth=5, alpha=.7, color=\".7\")\n",
    "    ax.legend(fontsize=20)\n",
    "    \n",
    "with dataviz.Figure(os.path.join(img_dir, \"downsample_figure_presentation.svg\"), figsize=(4* num_cols, 4*num_rows)) as fig:\n",
    "    ax = fig.add_subplot(num_rows, num_cols, 1)\n",
    "    for uID, df in rbps_to_plot_df.groupby(level=0):\n",
    "        rbp_name = df.index.get_level_values(level=\"RBP\")[0]\n",
    "\n",
    "        name = \"{}\".format(rbp_name)\n",
    "        ax.plot(df[reads], df.fraction_overlap, \n",
    "                label=name, \n",
    "                alpha=.7, \n",
    "                linewidth=5)\n",
    "        ax.xaxis.set_major_formatter(xfmt)\n",
    "        ax.set_xlabel(\"Reads (M)\", fontsize=20)\n",
    "        ax.set_ylabel(\"Fraction Peaks\", fontsize=20)\n",
    "        ax.set_ylim(0,1)\n",
    "        ax.set_title(\"Fraction of\\nTotal Peaks Recovered\", fontsize=20)\n",
    "        sns.despine(ax=ax)\n",
    "        [tick.set_fontsize(18) for tick in ax.get_xticklabels()]\n",
    "        [tick.set_fontsize(18) for tick in ax.get_yticklabels()]\n",
    "\n",
    "    ax.legend(fontsize=20)\n",
    "    \n",
    "with dataviz.Figure(os.path.join(img_dir, \"downsample_figure_pub.svg\"), figsize=(2.5* num_cols, 2.5*num_rows)) as fig:\n",
    "    ax = fig.add_subplot(num_rows, num_cols, 1)\n",
    "    for uID, df in rbps_to_plot_df.groupby(level=0):\n",
    "        rbp_name = df.index.get_level_values(level=\"RBP\")[0]\n",
    "\n",
    "        name = \"{}\".format(rbp_name)\n",
    "        ax.plot(df[reads], df.fraction_overlap, \n",
    "                label=name, \n",
    "                alpha=.7, \n",
    "                linewidth=3)\n",
    "        ax.xaxis.set_major_formatter(xfmt)\n",
    "        ax.set_xlabel(\"Reads (M)\", fontsize=8)\n",
    "        ax.set_ylabel(\"Fraction Peaks\", fontsize=8)\n",
    "        ax.set_ylim(0,1)\n",
    "        ax.set_title(\"Fraction of Total Peaks Recovered\", fontsize=8)\n",
    "        sns.despine(ax=ax)\n",
    "        [tick.set_fontsize(8) for tick in ax.get_xticklabels()]\n",
    "        [tick.set_fontsize(8) for tick in ax.get_yticklabels()]\n",
    "\n",
    "    ax.legend(fontsize=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate Entropy per recovered peak"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# merged_data.index = merged_data.index.rename(['uID', 'Cell line', 'RBP', 'rep', 'fraction'])\n",
    "# merged_data = merged_data.swaplevel(\"Cell line\", \"RBP\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "result = []\n",
    "for name, df in tqdm(list(merged_data.groupby(level=[\"uID\", \"RBP\", \"Cell line\", \"rep\"]))):\n",
    "    result.append(when_peak_recovered_beds(df))\n",
    "    \n",
    "appearing_peaks = pd.concat(result)\n",
    "appearing_peaks = appearing_peaks.rename(\"appearing_peaks\")\n",
    "merged_data = pd.concat([merged_data, appearing_peaks], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "entropy_header = [\"full_name\", \"ip_reads\", \"input_reads\", \"p_val\", \"fold_change\", \"something\", \"enrichment\", \"log10_p_val\", \"log2_fold_change\", 'entropy']\n",
    "entropy_header = annotated_bedtool_header + entropy_header\n",
    "\n",
    "def get_total_entropy(fn):\n",
    "    df = pd.read_table(fn, names=entropy_header, \n",
    "              header=None).entropy.sum()\n",
    "    return df\n",
    "\n",
    "merged_data['entropy'] = merged_data.appearing_peaks.progress_apply(get_total_entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def group_fraction(series):\n",
    "    return  np.cumsum(series / series.sum())\n",
    "\n",
    "merged_data['fraction_entropy'] = merged_data.entropy.groupby(level=[\"uID\", \"RBP\", \"Cell line\", \"rep\"]).transform(group_fraction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "total_datasets = len(merged_data.groupby(level=[\"uID\", \"RBP\", \"Cell line\"]))\n",
    "num_cols = 4\n",
    "num_rows = (total_datasets / 4) + 1\n",
    "count = 0\n",
    "\n",
    "with dataviz.Figure(os.path.join(img_dir, \"entropy_curve.svg\"), figsize=(4* num_cols, 4*num_rows)) as fig:\n",
    "    for name, df in merged_data.groupby(level=[\"uID\", \"RBP\", \"Cell line\"]):\n",
    "        count += 1\n",
    "        ax = fig.add_subplot(num_rows, num_cols, count)\n",
    "\n",
    "        for rep, dataset_df in df.groupby(level=\"rep\"):\n",
    "            legend_name = \"{} {}\".format(uID, name[1])\n",
    "            ax.plot(dataset_df[reads], dataset_df.fraction_entropy, \n",
    "                    label=legend_name, \n",
    "                    alpha=.7, \n",
    "                    linewidth=3)\n",
    "            ax.xaxis.set_major_formatter(xfmt)\n",
    "            ax.set_xlabel(\"Reads (M)\")\n",
    "            ax.set_ylabel(\"Fraction Observed Entropy\")\n",
    "            ax.set_ylim(0,1)\n",
    "            sns.despine(ax=ax)\n",
    "        ax.legend(fontsize=18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#rbps_to_plot = [\"204\", \"205\", \"331\", \"242\", \"240\"]\n",
    "rbps_to_plot = [\"204\", \"242\", \"289\"]\n",
    "\n",
    "rbps_to_plot_df = merged_data.loc[rbps_to_plot]\n",
    "rbps_to_plot_df = rbps_to_plot_df.xs(\"rep1\", level=\"rep\")\n",
    "\n",
    "total_datasets = len(rbps_to_plot_df.groupby(level=0))\n",
    "num_cols = 1\n",
    "num_rows = 1\n",
    "count = 0\n",
    "\n",
    "with dataviz.Figure(os.path.join(img_dir, \"downsample_figure_entropy_pub_fraction_presentation.svg\"), figsize=(5* num_cols, 5*num_rows)) as fig:\n",
    "    ax = fig.add_subplot(num_rows, num_cols, 1)\n",
    "    for name, df in rbps_to_plot_df.groupby(level=[\"uID\", \"RBP\", \"Cell line\"]):\n",
    "        legend_name = \"{}\".format(name[1])\n",
    "        ax.plot(df.index.get_level_values(\"fraction\"), df.fraction_entropy, \n",
    "                label=legend_name, \n",
    "                alpha=.7, \n",
    "                linewidth=5)\n",
    "        ax.set_xlabel(\"Fraction Downsampled\", fontsize=20)\n",
    "        ax.set_ylabel(\"Fraction Entropy\", fontsize=20)\n",
    "        ax.set_ylim(0,1)\n",
    "        ax.set_title(\"Fraction of Entropy Recovered\", fontsize=20)\n",
    "        sns.despine(ax=ax)\n",
    "        [tick.set_fontsize(18) for tick in ax.get_xticklabels()]\n",
    "        [tick.set_fontsize(18) for tick in ax.get_yticklabels()]\n",
    "\n",
    "    ax.legend(fontsize=20)\n",
    "    \n",
    "with dataviz.Figure(os.path.join(img_dir, \"downsample_figure_entropy_pub_presentation.svg\"), figsize=(5* num_cols, 5*num_rows)) as fig:\n",
    "    ax = fig.add_subplot(num_rows, num_cols, 1)\n",
    "    for name, df in rbps_to_plot_df.groupby(level=[\"uID\", \"RBP\", \"Cell line\"]):\n",
    "        legend_name = \"{}\".format(name[1])\n",
    "        ax.plot(df[reads], df.fraction_entropy, \n",
    "                label=legend_name, \n",
    "                alpha=.7, \n",
    "                linewidth=5)\n",
    "        ax.xaxis.set_major_formatter(xfmt)\n",
    "        ax.set_xlabel(\"Reads (M)\", fontsize=20)\n",
    "        ax.set_ylabel(\"Fraction Entropy\", fontsize=20)\n",
    "        ax.set_ylim(0,1)\n",
    "        ax.set_title(\"Fraction of Entropy Recovered\", fontsize=20)\n",
    "        sns.despine(ax=ax)\n",
    "        [tick.set_fontsize(18) for tick in ax.get_xticklabels()]\n",
    "        [tick.set_fontsize(18) for tick in ax.get_yticklabels()]\n",
    "\n",
    "    ax.legend(fontsize=20)\n",
    "    \n",
    "with dataviz.Figure(os.path.join(img_dir, \"downsample_figure_entropy_pub.svg\"), figsize=(2.5* num_cols, 2.5*num_rows)) as fig:\n",
    "    ax = fig.add_subplot(num_rows, num_cols, 1)\n",
    "    for name, df in rbps_to_plot_df.groupby(level=[\"uID\", \"RBP\", \"Cell line\"]):\n",
    "        legend_name = \"{}\".format(name[1])\n",
    "        ax.plot(df[reads], df.fraction_entropy, \n",
    "                label=legend_name, \n",
    "                alpha=.7, \n",
    "                linewidth=3)\n",
    "        ax.xaxis.set_major_formatter(xfmt)\n",
    "        ax.set_xlabel(\"Reads (M)\", fontsize=8)\n",
    "        ax.set_ylabel(\"Fraction Entropy\", fontsize=8)\n",
    "        ax.set_ylim(0,1)\n",
    "        ax.set_title(\"Fraction of Entropy Recovered\", fontsize=8)\n",
    "        sns.despine(ax=ax)\n",
    "        [tick.set_fontsize(8) for tick in ax.get_xticklabels()]\n",
    "        [tick.set_fontsize(8) for tick in ax.get_yticklabels()]\n",
    "\n",
    "    ax.legend(fontsize=8)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How many RBPs recover by fraction, and sequencing depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = (12 / 22.) * 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y = (6 / 22.) * 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "z= (4 / 22.) * 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x+y+z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fraction_above_cutoff = merged_data[merged_data.fraction_overlap > .90]\n",
    "fraction_above_cutoff['fraction'] = fraction_above_cutoff.index.get_level_values(\"fraction\")\n",
    "fraction_above_cutoff = fraction_above_cutoff.groupby(\"fraction\").count().fraction_overlap\n",
    "fraction_above_cutoff = fraction_above_cutoff / fraction_above_cutoff.loc[1.0]\n",
    "\n",
    "with dataviz.Figure(os.path.join(img_dir, \"fraction_peaks_recovered_presentation.svg\"), figsize=(4* num_cols, 4*num_rows)) as fig:\n",
    "    ax = fig.add_subplot(num_rows, num_cols, 1)\n",
    "    ax.plot(fraction_above_cutoff.index, fraction_above_cutoff.values, linewidth=5, alpha=.7)\n",
    "    ax.set_xlabel(\"Fraction of Reads Downsampled\", fontsize=20)\n",
    "    ax.set_ylabel(\"Fraction RBPs\", fontsize=20)\n",
    "    ax.set_title(\"Fraction of RBPs with\\n>90% of peaks recovered\", fontsize=20)\n",
    "    sns.despine(ax=ax)\n",
    "    ax.set_xlim(0,1)\n",
    "    ax.set_ylim(0,1)\n",
    "    [tick.set_fontsize(18) for tick in ax.get_xticklabels()]\n",
    "    [tick.set_fontsize(18) for tick in ax.get_yticklabels()]\n",
    "    \n",
    "with dataviz.Figure(os.path.join(img_dir, \"fraction_peaks_recovered.svg\"), figsize=(2.5* num_cols, 2.5*num_rows)) as fig:\n",
    "    ax = fig.add_subplot(num_rows, num_cols, 1)\n",
    "    ax.plot(fraction_above_cutoff.index, fraction_above_cutoff.values, linewidth=3, alpha=.7)\n",
    "    ax.set_xlabel(\"Fraction of Reads Downsampled\", fontsize=8)\n",
    "    ax.set_ylabel(\"Fraction RBPs\", fontsize=8)\n",
    "    ax.set_title(\"Fraction of RBPs with\\n>90% of peaks recovered\", fontsize=8)\n",
    "    sns.despine(ax=ax)\n",
    "    ax.set_xlim(0,1)\n",
    "    ax.set_ylim(0,1)\n",
    "    [tick.set_fontsize(8) for tick in ax.get_xticklabels()]\n",
    "    [tick.set_fontsize(8) for tick in ax.get_yticklabels()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Lets do this by read depth as well\n",
    "read_increments = 10000\n",
    "reads_above_cutoff = merged_data[merged_data.fraction_overlap > .90]\n",
    "\n",
    "#I try to group reads and count at what read depth I recover more than 90% of peaks\n",
    "reads_above_cutoff['fraction_reads'] = np.floor(reads_above_cutoff[reads].astype(int) / read_increments)\n",
    "reads_above_cutoff = reads_above_cutoff.groupby(level=[0,1]).first()\n",
    "max_reads = int(np.max(reads_above_cutoff.fraction_reads))\n",
    "reads_above_cutoff_fraction = np.zeros(max_reads)\n",
    "\n",
    "for value in reads_above_cutoff.fraction_reads:\n",
    "    reads_above_cutoff_fraction[int(value):] += 1\n",
    "reads_above_cutoff_fraction = reads_above_cutoff_fraction / float(reads_above_cutoff_fraction[-1])\n",
    "\n",
    "\n",
    "with dataviz.Figure(os.path.join(img_dir, \"read_number_peaks_recovered.svg\"), figsize=(2.5* num_cols, 2.5*num_rows)) as fig:\n",
    "    ax = fig.add_subplot(num_rows, num_cols, 1)\n",
    "    ax.plot(np.arange(0, len(reads_above_cutoff_fraction) * read_increments, read_increments), reads_above_cutoff_fraction, linewidth=3, alpha=.7)\n",
    "    ax.set_xlabel(\"Reads (M)\", fontsize=8)\n",
    "    ax.set_ylabel(\"Fraction RBPs\", fontsize=8)\n",
    "    ax.set_title(\"Fraction of RBPs with\\n>90% of peaks recovered\", fontsize=8)\n",
    "    ax.xaxis.set_major_formatter(xfmt)\n",
    "    sns.despine(ax=ax)\n",
    "    [tick.set_fontsize(8) for tick in ax.get_xticklabels()]\n",
    "    [tick.set_fontsize(8) for tick in ax.get_yticklabels()]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Do the above except with Entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fraction_above_cutoff = merged_data[merged_data.fraction_entropy > .90]\n",
    "fraction_above_cutoff = fraction_above_cutoff.groupby(level=\"fraction\").count().fraction_entropy\n",
    "fraction_above_cutoff = fraction_above_cutoff / fraction_above_cutoff.loc[1.0]\n",
    "fraction_above_cutoff = fraction_above_cutoff.sort_index()\n",
    "\n",
    "with dataviz.Figure(os.path.join(img_dir, \"fraction_entropy_recovered_presentation.svg\"), figsize=(3.5* num_cols, 3.5*num_rows)) as fig:\n",
    "    ax = fig.add_subplot(num_rows, num_cols, 1)\n",
    "    ax.plot(fraction_above_cutoff.index, fraction_above_cutoff.values, linewidth=3, alpha=.7)\n",
    "    ax.set_xlabel(\"Fraction\\nreads downsampled\", fontsize=20)\n",
    "    ax.set_ylabel(\"Fraction RBPs\", fontsize=20)\n",
    "    ax.set_title(\"Fraction RBPs with >90%\\nentropy recovered\", fontsize=20)\n",
    "    sns.despine(ax=ax)\n",
    "    ax.set_xticks(np.arange(0,1.2, .3))\n",
    "    ax.set_yticks(np.arange(0,1.2, .2))\n",
    "\n",
    "    [tick.set_fontsize(20) for tick in ax.get_xticklabels()]\n",
    "    [tick.set_fontsize(20) for tick in ax.get_yticklabels()]\n",
    "    \n",
    "with dataviz.Figure(os.path.join(img_dir, \"fraction_entropy_recovered.svg\"), figsize=(2.5* num_cols, 2.5*num_rows)) as fig:\n",
    "    ax = fig.add_subplot(num_rows, num_cols, 1)\n",
    "    ax.plot(fraction_above_cutoff.index, fraction_above_cutoff.values, linewidth=3, alpha=.7)\n",
    "    ax.set_xlabel(\"Fraction of reads downsampled\", fontsize=8)\n",
    "    ax.set_ylabel(\"Fraction RBPs\", fontsize=8)\n",
    "    ax.set_title(\"Fraction of RBPs with\\n>90% entropy recovered\", fontsize=8)\n",
    "    sns.despine(ax=ax)\n",
    "    \n",
    "    [tick.set_fontsize(8) for tick in ax.get_xticklabels()]\n",
    "    [tick.set_fontsize(8) for tick in ax.get_yticklabels()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.where(reads_above_cutoff_fraction > .80)[0][0] * read_increments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Lets do this by read depth as well\n",
    "read_increments = 10000\n",
    "reads_above_cutoff = merged_data[merged_data.fraction_entropy > .90]\n",
    "\n",
    "#I try to group reads and count at what read depth I recover more than 90% of peaks\n",
    "reads_above_cutoff['fraction_reads'] = np.floor(reads_above_cutoff[reads].astype(int) / read_increments)\n",
    "reads_above_cutoff = reads_above_cutoff.groupby(level=[0,1]).first()\n",
    "max_reads = int(np.max(reads_above_cutoff.fraction_reads))\n",
    "reads_above_cutoff_fraction = np.zeros(max_reads)\n",
    "\n",
    "for value in reads_above_cutoff.fraction_reads:\n",
    "    reads_above_cutoff_fraction[int(value):] += 1\n",
    "reads_above_cutoff_fraction = reads_above_cutoff_fraction / float(reads_above_cutoff_fraction[-1])\n",
    "\n",
    "with dataviz.Figure(os.path.join(img_dir, \"read_number_entropy_recovered_presentation.svg\"), figsize=(3.5* num_cols, 3.5*num_rows)) as fig:\n",
    "    ax = fig.add_subplot(num_rows, num_cols, 1)\n",
    "    ax.plot(np.arange(0, len(reads_above_cutoff_fraction) * read_increments, \n",
    "                      read_increments), reads_above_cutoff_fraction, linewidth=5, alpha=.7)\n",
    "    ax.set_xlabel(\"Reads (M)\", fontsize=20)\n",
    "    ax.set_ylabel(\"Fraction RBPs\", fontsize=20)\n",
    "    ax.set_title(\"Fraction RBPs with >90%\\n of entropy recovered\", fontsize=20)\n",
    "    ax.axvline(3000000, linewidth=5, color=\".7\", linestyle=\"--\")\n",
    "    ax.xaxis.set_major_formatter(xfmt_int)\n",
    "    sns.despine(ax=ax)\n",
    "    #ax.set_xticks(np.arange(0,1.2, .3))\n",
    "    ax.set_yticks(np.arange(0,1.2, .2))\n",
    "\n",
    "    [tick.set_fontsize(20) for tick in ax.get_xticklabels()]\n",
    "    [tick.set_fontsize(20) for tick in ax.get_yticklabels()]\n",
    "    \n",
    "with dataviz.Figure(os.path.join(img_dir, \"read_number_entropy_recovered.svg\"), figsize=(2.5* num_cols, 2.5*num_rows)) as fig:\n",
    "    ax = fig.add_subplot(num_rows, num_cols, 1)\n",
    "    ax.plot(np.arange(0, len(reads_above_cutoff_fraction) * read_increments, read_increments), reads_above_cutoff_fraction, linewidth=3, alpha=.7)\n",
    "    ax.set_xlabel(\"Reads (M)\", fontsize=8)\n",
    "    ax.set_ylabel(\"Fraction RBPs\", fontsize=8)\n",
    "    ax.set_title(\"Fraction of RBPs with\\n>90% of entropy recovered\", fontsize=8)\n",
    "    ax.axvline(3000000, linewidth=3, color=\".7\", linestyle=\"--\")\n",
    "    ax.xaxis.set_major_formatter(xfmt_int)\n",
    "    sns.despine(ax=ax)\n",
    "    \n",
    "    [tick.set_fontsize(8) for tick in ax.get_xticklabels()]\n",
    "    [tick.set_fontsize(8) for tick in ax.get_yticklabels()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Lets do this by read depth as well\n",
    "read_increments = 10000\n",
    "reads_above_cutoff = initial_counts[initial_counts.fraction_entropy > .90]\n",
    "reads_above_cutoff = reads_above_cutoff.drop(1.0, level=\"fraction\")\n",
    "\n",
    "#I try to group reads and count at what read depth I recover more than 90% of peaks\n",
    "reads_above_cutoff['fraction_reads'] = np.floor(reads_above_cutoff[reads].astype(int) / read_increments)\n",
    "reads_above_cutoff = reads_above_cutoff.groupby(level=[0,1]).first()\n",
    "max_reads = int(np.max(reads_above_cutoff.fraction_reads))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with dataviz.Figure(os.path.join(img_dir, \"ect_vs_saturating_read_numbers.svg\"), figsize=(2.5* num_cols, 2.5*num_rows)) as fig:\n",
    "    ax = fig.add_subplot(num_rows, num_cols, 1)\n",
    "    ax.scatter(reads_above_cutoff.eCT, reads_above_cutoff[reads], linewidth=3, alpha=.7, s=2)\n",
    "    ax.set_xlabel(\"Reads (M)\", fontsize=8)\n",
    "    ax.set_ylabel(\"Fraction RBPs\", fontsize=8)\n",
    "    ax.set_title(\"Fraction of RBPs with\\n>90% of entropy recovered\", fontsize=8)\n",
    "    ax.yaxis.set_major_formatter(xfmt)\n",
    "    sns.despine(ax=ax)\n",
    "    ax.set_ylim(0)\n",
    "    [tick.set_fontsize(8) for tick in ax.get_xticklabels()]\n",
    "    [tick.set_fontsize(8) for tick in ax.get_yticklabels()]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These these are the datasets that saturate before the final downsample.  What I see here is very little corrleation between eCT and number of reads it takes saturate.  This is a good thing, it means saturation is independent of eCT, so we can make our model indepenent of eCT.  I don't think I'll even write about about it because its not the case.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RBPs don't overlap super well between bioloigcal replicates "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gene_df = pd.read_table(\"/home/gpratt/clipper/clipper/data/regions/hg19_HepG2_genes.bed\", \n",
    "                        names=['chrom', 'start', 'stop', 'name', 'score', 'strand'])\n",
    "gene_df['gene_length'] = gene_df.stop - gene_df.start\n",
    "gene_lengths = gene_df.groupby(\"chrom\").sum().gene_length\n",
    "gene_lengths.to_csv(\"hg19_HepG2_gene_lengths.txt\", sep=\"\\t\")\n",
    "\n",
    "gene_df = pd.read_table(\"/home/gpratt/clipper/clipper/data/regions/hg19_K562_genes.bed\", \n",
    "                        names=['chrom', 'start', 'stop', 'name', 'score', 'strand'])\n",
    "gene_df['gene_length'] = gene_df.stop - gene_df.start\n",
    "gene_lengths = gene_df.groupby(\"chrom\").sum().gene_length\n",
    "gene_lengths.to_csv(\"hg19_K562_gene_lengths.txt\", sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def overlap_between_reps(row):\n",
    "    rep1 = pybedtools.BedTool(row[('filtered_moderate', 'rep1')])\n",
    "    rep2 = pybedtools.BedTool(row[('filtered_moderate', 'rep2')])\n",
    "\n",
    "    return len(rep1.intersect(rep2, u=True, s=True)) / float(len(rep1))\n",
    "\n",
    "def fisher_between_reps(row):\n",
    "    rep1 = pybedtools.BedTool(row[('filtered_moderate', 'rep1')])\n",
    "    rep2 = pybedtools.BedTool(row[('filtered_moderate', 'rep2')])\n",
    "\n",
    "    if row.name[1] == \"HepG2\":\n",
    "        species = \"hg19_HepG2_gene_lengths.txt\"\n",
    "    elif row.name[1] == \"K562\":\n",
    "        species = \"hg19_K562_gene_lengths.txt\"\n",
    "    species = \"hg19_gene_length_test.txt\"\n",
    "    return rep1.fisher(rep2, g=species)\n",
    "\n",
    "root_dir = \"/home/gpratt/projects/encode/analysis/peak_reanalysis_v14/assigned/\"\n",
    "def shuffle_between_reps(row):\n",
    "    rep1 = row[('filtered_moderate', 'rep1')]\n",
    "    rep2 = row[('filtered_moderate', 'rep2')]\n",
    "\n",
    "    overlap = []\n",
    "    for x in range(3):\n",
    "        rand_1 = os.path.join(root_dir, os.path.basename(rep1) + \".clip_formatted.bed.all.rand.{}.BED\".format(str(x)))\n",
    "        rand_2 = os.path.join(root_dir, os.path.basename(rep2) + \".clip_formatted.bed.all.rand.{}.BED\".format(str(x)))\n",
    "        \n",
    "        rand_1 = pybedtools.BedTool(rand_1)\n",
    "        rand_2 = pybedtools.BedTool(rand_2)\n",
    "\n",
    "        overlap.append(len(rand_1.intersect(rand_2, u=True, s=True)) / float(len(rand_1)))\n",
    "    return overlap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rep_comparisons = merged_data.xs(1.0, level=\"fraction\").unstack()\n",
    "rep_comparisons['fraction_overlap'] = rep_comparisons.apply(overlap_between_reps, axis=1)\n",
    "\n",
    "rep_comparisons['fisher'] = rep_comparisons.apply(fisher_between_reps, axis=1)\n",
    "rep_comparisons['fisher_two_tail_pvalues'] = rep_comparisons['fisher'].apply(lambda x: x.two_tail)\n",
    "\n",
    "#this is broken at the moment\n",
    "# rep_comparisons['shuffle'] = rep_comparisons.apply(shuffle_between_reps, axis=1)\n",
    "# rep_comparisons['shuffle_mean'] = rep_comparisons['shuffle'].apply(lambda x: np.mean(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with dataviz.Figure(os.path.join(img_dir, \"peak_overlap.svg\"), figsize=(4* 1, 4*1)) as fig:\n",
    "    ax = fig.add_subplot(1,1,1)\n",
    "    sns.violinplot(rep_comparisons[\"fraction_overlap\", \"rep1\"], cut=True, ax=ax, orient='v')\n",
    "    sns.despine(ax=ax)\n",
    "    ax.set_xlabel(\"RBPs\")\n",
    "    ax.set_ylabel(\"Fraction overlapping between technical replicates\")\n",
    "    ax.set_title(\"Peak Overlap Between\\nBiological Replicates\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test question about shitty peaks\n",
    "1. Of the peaks that overlap how much does the fold change, change, how much does the p-value change?\n",
    "2. Of the peaks that don't overlap and are found by deeper sequencing, there are two options,  Are we finding shittier peaks? Or are we finding peaks in more lowly expressed Genes?  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Addressing fold change question\n",
    "\n",
    "This data isn't interesting, commenting until it becomes important enough to update with new layout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def intersect_to_df(tool_1, tool_2):\n",
    "#     intersected_bed = tool_1.intersect(tool_2, s=True, wo=True)\n",
    "\n",
    "#     intersected_df = intersected_bed.to_dataframe(names=['fraction_chrom', 'fraction_start', \n",
    "#                                         'fraction_stop', 'fraction_pval', \n",
    "#                                         'fraction_log2fc', 'fraction_strand',\n",
    "#                                        'full_chrom', 'full_start', \n",
    "#                                         'full_stop', 'full_pval', \n",
    "#                                         'full_log2fc', 'full_strand',\n",
    "#                                        'full_annotation', 'full_gene', 'foo'])\n",
    "#     intersected_df = intersected_df.groupby(['fraction_chrom', 'fraction_start', 'fraction_stop']).first()\n",
    "#     return intersected_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# total_bed = pybedtools.BedTool(merged_data.ix[2].rep1_filtered_moderate)\n",
    "# fraction_bed = pybedtools.BedTool(make_and_filter_clipper_moderate(fully_downsampled_data.ix[2].CLIP_rep1_01))\n",
    "# intersected_df = intersect_to_df(fraction_bed, total_bed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# len(total_bed), len(fraction_bed), len(intersected_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# num_cols = 1\n",
    "# num_rows = 2\n",
    "# with dataviz.Figure(os.path.join(img_dir, \"peak_overlap.svg\"), figsize=(4* num_rows, 4*num_cols)) as fig:\n",
    "#     ax = fig.add_subplot(num_cols,num_rows,1)\n",
    "    \n",
    "    \n",
    "#     ax.scatter(intersected_df.fraction_log2fc, intersected_df.full_log2fc, alpha=.7, color='.2')\n",
    "    \n",
    "#     linregress = scipy.stats.linregress(intersected_df.fraction_log2fc, intersected_df.full_log2fc)\n",
    "#     fit = np.poly1d([linregress.slope, linregress.intercept])\n",
    "#     ax.plot([min(intersected_df.fraction_log2fc), max(intersected_df.fraction_log2fc)],\n",
    "#             [fit(min(intersected_df.fraction_log2fc)), fit(max(intersected_df.fraction_log2fc))],\n",
    "#            linewidth=3, alpha=.7)\n",
    "    \n",
    "#     sns.despine(ax=ax)\n",
    "#     ax.set_xlabel(\"Fraction log2 fc\")\n",
    "#     ax.set_ylabel(\"Total log2 fc\")\n",
    "#     ax.set_xlim(2,10)\n",
    "#     ax.set_ylim(2,10)\n",
    "    \n",
    "#     ax = fig.add_subplot(num_cols,num_rows,2)\n",
    "#     ax.scatter(intersected_df.fraction_pval, intersected_df.full_pval, alpha=.7, color='.2')\n",
    "    \n",
    "#     linregress = scipy.stats.linregress(intersected_df.fraction_pval, intersected_df.full_pval)\n",
    "#     fit = np.poly1d([linregress.slope, linregress.intercept])\n",
    "#     ax.plot([min(intersected_df.fraction_pval), max(intersected_df.fraction_pval)],\n",
    "#             [fit(min(intersected_df.fraction_pval)), fit(max(intersected_df.fraction_pval))],\n",
    "#             linewidth=3, alpha=.7)\n",
    "    \n",
    "#     sns.despine(ax=ax)\n",
    "#     ax.set_xlabel(\"Fraction pval\")\n",
    "#     ax.set_ylabel(\"Total pval\")\n",
    "#     ax.set_xlim(0,400)\n",
    "#     ax.set_ylim(0,400)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This isn't a really good way to look at the data because it starts digging into issues with clipper.  Thats not the question we are asking.  We want to know what do the undiscovered peaks look like, not why were they undiscovered.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# print scipy.stats.linregress(intersected_df.fraction_pval, intersected_df.full_pval)\n",
    "# print scipy.stats.linregress(intersected_df.fraction_log2fc, intersected_df.full_log2fc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Need to explain why the slope is less than 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So this is telling me that my p-value and fold change isn't really changing.  Do I want to ask how many are lost because they are detected, but not significant, and how many are lost because they aren't detected at all.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# total_bed = pybedtools.BedTool(fully_downsampled_data.ix[1].rep1_filtered_moderate)\n",
    "# fraction_bed = pybedtools.BedTool(fully_downsampled_data.ix[1].CLIP_rep1_01)\n",
    "\n",
    "# unfiltered = intersect_to_df(fraction_bed, total_bed)\n",
    "# no_pass = unfiltered[~unfiltered.index.isin(intersected_df.index)]\n",
    "# (no_pass.fraction_pval > 3) & (no_pass.fraction_log2fc > 3)\n",
    "\n",
    "# print len(no_pass), sum(no_pass.fraction_log2fc > 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Of the peaks not called significant all but two of them are not called significant due to a low p-value.  Not a low fold change"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I need to change approaches and look at peaks that weren't identified, agnostic of how they weren't identified, then run clip analysis on the result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Of the peaks that I don't identify quickly, what is the quality of motif recovery?\n",
    "\n",
    "This is an attempt to run homer, that I don't like / need anymore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# total_bed = pybedtools.BedTool(fully_downsampled_data.ix[1].rep1_filtered_moderate)\n",
    "# fraction_bed = pybedtools.BedTool(make_and_filter_clipper_moderate(fully_downsampled_data.ix[1].CLIP_rep1_01))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Of the peaks that aren't overlapping, what is the expression level of the RNA thats being detected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# #K562 Expression values\n",
    "# foo = urllib.urlretrieve(\"https://www.encodeproject.org/files/ENCFF286GLL/@@download/ENCFF286GLL.tsv\", \"/projects/ps-yeolab3/encode/analysis/rnaseq_bams_v2/ENCFF286GLL.tsv\")\n",
    "# foo = urllib.urlretrieve(\"https://www.encodeproject.org/files/ENCFF986DBN/@@download/ENCFF986DBN.tsv\", \"/projects/ps-yeolab3/encode/analysis/rnaseq_bams_v2/ENCFF986DBN.tsv\")\n",
    "\n",
    "# #HepG2 Expression values\n",
    "# foo = urllib.urlretrieve(\"https://www.encodeproject.org/files/ENCFF533XPJ/@@download/ENCFF533XPJ.tsv\", \"/projects/ps-yeolab3/encode/analysis/rnaseq_bams_v2/ENCFF533XPJ.tsv\")\n",
    "# foo = urllib.urlretrieve(\"https://www.encodeproject.org/files/ENCFF321JIT/@@download/ENCFF321JIT.tsv\", \"/projects/ps-yeolab3/encode/analysis/rnaseq_bams_v2/ENCFF321JIT.tsv\")\n",
    "\n",
    "# #K562 bams\n",
    "# foo = urllib.urlretrieve(\"https://www.encodeproject.org/files/ENCFF131IST/@@download/ENCFF131IST.bam\", \"/projects/ps-yeolab3/encode/analysis/rnaseq_bams_v2/ENCFF131IST.bam\")\n",
    "# foo = urllib.urlretrieve(\"https://www.encodeproject.org/files/ENCFF726SMY/@@download/ENCFF726SMY.bam\", \"/projects/ps-yeolab3/encode/analysis/rnaseq_bams_v2/ENCFF726SMY.bam\")\n",
    "\n",
    "# #HepG2 bams\n",
    "# foo = urllib.urlretrieve(\"https://www.encodeproject.org/files/ENCFF770XVY/@@download/ENCFF770XVY.bam\", \"/projects/ps-yeolab3/encode/analysis/rnaseq_bams_v2/ENCFF770XVY.bam\")\n",
    "# foo = urllib.urlretrieve(\"https://www.encodeproject.org/files/ENCFF002PXG/@@download/ENCFF002PXG.bam\", \"/projects/ps-yeolab3/encode/analysis/rnaseq_bams_v2/ENCFF002PXG.bam\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "k562_rep1 = pd.read_table(\"/projects/ps-yeolab3/encode/analysis/rnaseq_bams_v2/ENCFF286GLL.tsv\")\n",
    "k562_rep2 = pd.read_table(\"/projects/ps-yeolab3/encode/analysis/rnaseq_bams_v2/ENCFF986DBN.tsv\")\n",
    "\n",
    "hepg2_rep1 = pd.read_table(\"/projects/ps-yeolab3/encode/analysis/rnaseq_bams_v2/ENCFF533XPJ.tsv\")\n",
    "hepg2_rep2 = pd.read_table(\"/projects/ps-yeolab3/encode/analysis/rnaseq_bams_v2/ENCFF321JIT.tsv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_columns = [u'chrom', u'start', u'end', u'name', u'score', u'strand', u'annotation',\n",
    "       u'gene_id', u'transcript_id(s)', u'length', u'effective_length',\n",
    "       u'expected_count', u'TPM', u'FPKM', u'posterior_mean_count',\n",
    "       u'posterior_standard_deviation_of_count', u'pme_TPM', u'pme_FPKM',\n",
    "       u'TPM_ci_lower_bound', u'TPM_ci_upper_bound', u'FPKM_ci_lower_bound',\n",
    "       u'FPKM_ci_upper_bound', u'overlap_type']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def calculate_ntile(df, ntile=4):\n",
    "    \"Takes a dataframe and returns a multi-index dataframe where the first index is now the quartile\"\n",
    "    ntiles = np.arange(0, len(df) + 1, len(df) / ntile)\n",
    "\n",
    "    results = {}\n",
    "    for x, (start, finish) in enumerate(zip(ntiles, ntiles[1:])):\n",
    "        results[x + 1] = df.iloc[start:finish]\n",
    "\n",
    "    quartiled = pd.concat(results, names=['quartile', 'id'])\n",
    "    quartiled['quartile'] = quartiled.index.get_level_values(level=0)\n",
    "    return quartiled\n",
    "\n",
    "\n",
    "hepg2_rep1_tpm_sorted = hepg2_rep1.sort_values(by=\"TPM\", axis=0)\n",
    "hepg2_rep1_tpm_sorted = hepg2_rep1_tpm_sorted[hepg2_rep1_tpm_sorted.TPM > 0]\n",
    "hepg2_rep1_tpm_sorted = calculate_ntile(hepg2_rep1_tpm_sorted)\n",
    "\n",
    "k562_rep1_tpm_sorted = k562_rep1.sort_values(by=\"TPM\", axis=0)\n",
    "k562_rep1_tpm_sorted = k562_rep1_tpm_sorted[k562_rep1_tpm_sorted.TPM > 0]\n",
    "k562_rep1_tpm_sorted = calculate_ntile(k562_rep1_tpm_sorted)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_recovered_peaks(df):\n",
    "    cell_line = df.index.get_level_values(level=\"Cell line\")[0]\n",
    "    if cell_line == \"HepG2\":\n",
    "        cell_expression = hepg2_rep1_tpm_sorted\n",
    "    elif cell_line == \"K562\":\n",
    "        cell_expression = k562_rep1_tpm_sorted\n",
    "    else:\n",
    "        print \"error\"\n",
    "    \n",
    "    result = {}\n",
    "    for name, row in df.iterrows():\n",
    "        result[name] = pd.read_table(row.appearing_peaks, names=entropy_header)\n",
    "        result[name]['reads'] = row.reads\n",
    "    result = pd.concat(result)\n",
    "    result.index.names = ['uID', \"RBP\", \"Cell line\", \"rep\", \"fraction\", \"peak\"]\n",
    "\n",
    "    result = result.reset_index()\n",
    "    peaks_and_expression = pd.merge(result, cell_expression, \n",
    "         left_on='gene_id',\n",
    "         right_on='gene_id', how=\"left\")\n",
    "    return peaks_and_expression\n",
    "\n",
    "def plot_peak_expression_differences(df_rep1, df_rep2, out_file):\n",
    "    \"\"\"plots the expression of each gene peaks are bound to in both rep1 and rep2\"\"\"\n",
    "    num_cols = 1\n",
    "    num_rows = 2\n",
    "\n",
    "    df_rep1 = df_rep1.sort_values(\"fraction\")\n",
    "    df_rep2 = df_rep2.sort_values(\"fraction\")\n",
    "\n",
    "    with dataviz.Figure(os.path.join(img_dir, out_file), figsize=(4* num_rows, 4*num_cols)) as fig:\n",
    "        ax = fig.add_subplot(num_cols,num_rows,1)\n",
    "        sns.boxplot(x=\"TPM\", y=\"fraction\", data=df_rep1, ax=ax, orient='h')\n",
    "        ax.set(xscale=\"log\")\n",
    "        sns.despine(ax=ax)\n",
    "        ax.set_title(\"Gene Expression of Peaks Rep1\")\n",
    "        ax.set_ylabel(\"Fraction Peaks Recovered\")\n",
    "        ax.set_yticklabels(np.arange(.1,1.1, .1))\n",
    "\n",
    "        ax = fig.add_subplot(num_cols,num_rows,2)\n",
    "        sns.boxplot(x=\"TPM\", y=\"fraction\", data=df_rep2, ax=ax, orient='h')\n",
    "        ax.set(xscale=\"log\")\n",
    "        sns.despine(ax=ax)\n",
    "        ax.set_title(\"Gene Expression of Peaks Rep 2\")\n",
    "        ax.set_ylabel(\"Fraction Peaks Recovered\")\n",
    "        ax.set_yticklabels(np.arange(.1,1.1, .1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "grp = merged_data.groupby(level=[\"uID\", \"RBP\", 'Cell line',  \"rep\"])\n",
    "df_rep1 = grp.get_group(('204', 'RBFOX2', 'HepG2', \"rep1\"))\n",
    "df_rep2 = grp.get_group(('204', 'RBFOX2', 'HepG2', \"rep2\"))\n",
    "\n",
    "\n",
    "df_rep1 = get_recovered_peaks(df_rep1)\n",
    "df_rep2 = get_recovered_peaks(df_rep2)\n",
    "\n",
    "plot_peak_expression_differences(df_rep1, df_rep2, \"rbfox2_peak_expression.svg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_rep1_groups = df_rep1.groupby(\"fraction\")\n",
    "df_rep2_groups = df_rep2.groupby(\"fraction\")\n",
    "\n",
    "print \"rep1 tests\"\n",
    "for group1, group2 in zip(sorted(df_rep1_groups.groups.keys())[:-1], sorted(df_rep1_groups.groups.keys())[1:]):\n",
    "    print group1, group2, scipy.stats.mannwhitneyu(df_rep1_groups.get_group(group1).TPM, df_rep1_groups.get_group(group2).TPM) \n",
    "\n",
    "print\n",
    "print \"rep2 tests\"\n",
    "\n",
    "for group1, group2 in zip(sorted(df_rep2_groups.groups.keys())[:-1], sorted(df_rep2_groups.groups.keys())[1:]):\n",
    "    print group1, group2, scipy.stats.mannwhitneyu(df_rep2_groups.get_group(group1).TPM, df_rep2_groups.get_group(group2).TPM) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I've shown for RBFOX2 that gene expression doesn't matter when it comes to peak recovery.  The only stastical difference between peak quality is in the tranisition from the first to the second group.  Possibly can do all by all comparisons, but that doesn't make a bunch of sense.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try a Chi-Sq Test for RBFOX2, breaking up peaks into quartiles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Figure out this lack of gene expression pattern for the rest of the RBPs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mann_whitney_pvalues = {}\n",
    "chisq_pvalues = {}\n",
    "for name, df in tqdm(list(merged_data.groupby(level=[\"uID\", \"RBP\", \"Cell line\"]))): \n",
    "    uID, rbp, cell_line = name\n",
    "    \n",
    "    grp = df.groupby(level=\"rep\")\n",
    "    df_rep1 = get_recovered_peaks(grp.get_group(\"rep1\"))\n",
    "    df_rep2 = get_recovered_peaks(grp.get_group(\"rep2\"))\n",
    "\n",
    "    plot_peak_expression_differences(df_rep1, df_rep2, \"{}_{}_{}.expression_differences.svg\".format(uID, rbp, cell_line))\n",
    "    \n",
    "    df_rep1_groups = df_rep1.groupby(\"fraction\")\n",
    "    df_rep2_groups = df_rep2.groupby(\"fraction\")\n",
    "\n",
    "    chisq_pvalues[(uID, 'rep1')] = scipy.stats.chi2_contingency(df_rep1.groupby([\"fraction\", 'quartile']).count().FPKM.unstack().fillna(0))[1]\n",
    "    chisq_pvalues[(uID, 'rep2')] = scipy.stats.chi2_contingency(df_rep2.groupby([\"fraction\", 'quartile']).count().FPKM.unstack().fillna(0))[1]\n",
    "\n",
    "    for group1, group2 in zip(sorted(df_rep1_groups.groups.keys())[:-1], sorted(df_rep1_groups.groups.keys())[1:]):\n",
    "        pvalue = scipy.stats.mannwhitneyu(df_rep1_groups.get_group(group1).TPM, df_rep1_groups.get_group(group2).TPM).pvalue\n",
    "        mann_whitney_pvalues[(uID, 'rep1', group2)] = {\"pvalue\": pvalue}\n",
    "    \n",
    "    for group1, group2 in zip(sorted(df_rep2_groups.groups.keys())[:-1], sorted(df_rep2_groups.groups.keys())[1:]):\n",
    "        pvalue = scipy.stats.mannwhitneyu(df_rep2_groups.get_group(group1).TPM, df_rep2_groups.get_group(group2).TPM).pvalue\n",
    "        mann_whitney_pvalues[(uID, 'rep2', group2)] = {\"pvalue\": pvalue}\n",
    "mann_whitney_pvalues = pd.DataFrame(mann_whitney_pvalues).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mann_whitney_pvalues.groupby(level=2).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Easier than parsing the index, just add the fractions in as their own column\n",
    "mann_whitney_pvalues = mann_whitney_pvalues.reset_index()\n",
    "mann_whitney_pvalues.columns = ['uID', 'rep', 'fraction', 'pvalue']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#filtered_mann_whitney_pvalues = mann_whitney_pvalues[mann_whitney_pvalues.pvalue < (.05 / len(mann_whitney_pvalues))]\n",
    "filtered_mann_whitney_pvalues = mann_whitney_pvalues[mann_whitney_pvalues.pvalue < (.05)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "p_value_fraction = filtered_mann_whitney_pvalues.groupby(\"fraction\").count().pvalue / mann_whitney_pvalues.groupby(\"fraction\").count().pvalue\n",
    "p_value_fraction = p_value_fraction.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_cols = 1\n",
    "num_rows = 2\n",
    "with dataviz.Figure(os.path.join(\"/home/gpratt/Dropbox/encode_integration/qc_work/\", \"mann_p_value_test.svg\"), figsize=(4* num_rows, 4*num_cols)) as fig:\n",
    "    ax = fig.add_subplot(num_cols,num_rows,1)\n",
    "\n",
    "    sns.barplot(x=p_value_fraction.index, y=p_value_fraction.values, ax=ax)\n",
    "    sns.despine(ax=ax)\n",
    "    ax.set_xlabel(\"Fraction Peaks Recovered\")\n",
    "    ax.set_ylabel(\"Fraction of experiments with\\nsignificant change between downsampling steps\")\n",
    "    ax.set_title(\"All Experments with significant changes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lets look at the Chiseq values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "chisq_pvalues = pd.Series(chisq_pvalues)\n",
    "chisq_pvalues = pd.DataFrame(chisq_pvalues, columns=['p-value'])\n",
    "chisq_pvalues['neg_p_value'] = -1 * np.log10(chisq_pvalues[\"p-value\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "neg_05 = -1 * np.log10(.05 / len(chisq_pvalues))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "neg_05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with dataviz.Figure(os.path.join(\"/home/gpratt/Dropbox/encode_integration/qc_work/\", \"chisq_independence_test.svg\"), figsize=(4* num_rows, 4*num_cols)) as fig:\n",
    "    ax = fig.add_subplot(num_cols,num_rows,1)\n",
    "    \n",
    "    sns.distplot(chisq_pvalues[~np.isinf(chisq_pvalues['neg_p_value'])].neg_p_value, kde=False, ax=ax)\n",
    "    sns.despine(ax=ax)\n",
    "    ax.axvline(neg_05, linestyle=\"--\", alpha=.7, color=\".2\")\n",
    "    ax.set_xlim(0,)\n",
    "    ax.set_xlabel(\"-$log_{10}$ P Value\")\n",
    "    ax.set_ylabel(\"Number of Experiments\")\n",
    "    ax.set_title(\"Chi-Sq Indepdendence Test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Look at reads per peak vs RPKM of Gene"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "peaks = pd.read_table(total_bedtools.ix[('204', \"RBFOX2\", \"HepG2\", \"rep1\")]['entropy_tools'], names=entropy_header)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "peaks_and_expression = pd.merge(peaks, hepg2_rep1, how=\"left\", left_on=\"gene_id\", right_on=\"gene_id\")\n",
    "peaks_and_expression['normalized_read_count'] = peaks_and_expression.ip_reads / (peaks_and_expression.stop - peaks_and_expression.start)\n",
    "\n",
    "#I drop NA values because sometimes a peak is annotated to bind to two genes, in this case I don't make a decision which gene I choose\n",
    "peaks_and_expression = peaks_and_expression.dropna()\n",
    "\n",
    "#Because I'm plotting on a log plot, if the TPM is 0 I can't use it.  So I set the TPM to the next lowest expression level\n",
    "min_expression = peaks_and_expression[peaks_and_expression.TPM != 0].TPM.min()\n",
    "peaks_and_expression.ix[peaks_and_expression.TPM == 0, \"TPM\"] = min_expression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "scipy.stats.linregress(np.log10(peaks_and_expression.normalized_read_count),\n",
    "                        np.log10(peaks_and_expression.TPM)).rvalue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "num_rows = 1\n",
    "num_cols = 1\n",
    "with dataviz.Figure(os.path.join(img_dir, \"peaks_expression_correlation.svg\"), figsize=(4* num_rows, 4*num_cols)) as fig:\n",
    "    ax = fig.add_subplot(1,1,1)\n",
    "    #ax.scatter(peaks_and_expression.normalized_read_count, peaks_and_expression.TPM, alpha=.7, color=\".7\", rasterized=True)\n",
    "\n",
    "    sns.regplot(peaks_and_expression.TPM, \n",
    "                peaks_and_expression.normalized_read_count, \n",
    "                ci=None,\n",
    "                color=\".7\",\n",
    "                scatter_kws={\"alpha\":.7,},\n",
    "                line_kws={\"color\": 'b', \"alpha\": .7},\n",
    "                \n",
    "                ax=ax)\n",
    "        \n",
    "    ax.set_ylabel(\"$log_{10}$ Reads in Peaks\")\n",
    "    ax.set_xlabel(\"$log_{10}$ TPM\")\n",
    "    ax.set_xlim(.01, 100000)\n",
    "    ax.set_ylim(.01, 100000)\n",
    "    ax.set_yscale(\"log\", basey=10)\n",
    "    ax.set_xscale(\"log\", basex=10)\n",
    "    ax.set_title(\"RBFOX2 Gene Expression Correlation\")\n",
    "    sns.despine(ax=ax)\n",
    "    [tick.set_fontsize(8) for tick in ax.get_xticklabels()]\n",
    "    [tick.set_fontsize(8) for tick in ax.get_yticklabels()]\n",
    "    ax.legend()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "best_peaks_and_expression = peaks_and_expression.sort_values(\"normalized_read_count\", ascending=False)\n",
    "best_peaks_and_expression = best_peaks_and_expression.groupby(\"gene_id\").first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "linregress = scipy.stats.linregress(np.log10(best_peaks_and_expression.TPM), \n",
    "                                   np.log10(best_peaks_and_expression.normalized_read_count), \n",
    "                                  )\n",
    "poly = np.poly1d([linregress.slope, linregress.intercept])\n",
    "\n",
    "num_rows = 1\n",
    "num_cols = 1\n",
    "with dataviz.Figure(os.path.join(img_dir, \"peaks_expression_correlation_best_peak_presentation.svg\"), figsize=(5.0 * num_cols, 4.0*num_rows)) as fig:\n",
    "    ax = fig.add_subplot(num_rows, num_cols, 1)\n",
    "#     ax.scatter(best_peaks_and_expression.TPM,\n",
    "#                best_peaks_and_expression.normalized_read_count,  \n",
    "#                alpha=.7, color=\".7\", rasterized=True)\n",
    "    \n",
    "    hb = ax.hexbin(best_peaks_and_expression.TPM, best_peaks_and_expression.normalized_read_count, \n",
    "                   gridsize=25, \n",
    "                   bins='log',\n",
    "                   cmap='Greys',\n",
    "                   xscale='log',\n",
    "                   yscale='log',\n",
    "                   mincnt=.001\n",
    "                  )\n",
    "    cb = fig.colorbar(hb, ax=ax)\n",
    "    cb.set_label('log10(counts)', fontsize=20)\n",
    "\n",
    "    ax.plot(np.arange(.01, 100000),\n",
    "            np.power(10, poly(np.log10(np.arange(.01, 100000))))\n",
    "           )\n",
    "\n",
    "    ax.set_ylabel(\"Reads in Peaks\", fontsize=20)\n",
    "    ax.set_xlabel(\"TPM\", fontsize=20)\n",
    "    ax.set_xlim(.01, 100000)\n",
    "    ax.set_ylim(.01, 100000)\n",
    "    ax.set_yscale(\"log\", basey=10)\n",
    "    ax.set_xscale(\"log\", basex=10)\n",
    "    ax.set_title(\"RBFOX2 Expression Correlation\", fontsize=20)\n",
    "    \n",
    "    [tick.set_fontsize(16) for tick in ax.get_xticklabels()]\n",
    "    [tick.set_fontsize(16) for tick in ax.get_yticklabels()]\n",
    "    sns.despine(ax=ax)\n",
    "    \n",
    "num_rows = 1\n",
    "num_cols = 1\n",
    "with dataviz.Figure(os.path.join(img_dir, \"peaks_expression_correlation_best_peak.svg\"), figsize=(3.0 * num_cols, 2.5*num_rows)) as fig:\n",
    "    ax = fig.add_subplot(num_rows, num_cols, 1)\n",
    "#     ax.scatter(best_peaks_and_expression.TPM,\n",
    "#                best_peaks_and_expression.normalized_read_count,  \n",
    "#                alpha=.7, color=\".7\", rasterized=True)\n",
    "\n",
    "    hb = ax.hexbin(best_peaks_and_expression.TPM, best_peaks_and_expression.normalized_read_count, \n",
    "                   gridsize=25, \n",
    "                   bins='log',\n",
    "                   cmap='Greys',\n",
    "                   xscale='log',\n",
    "                   yscale='log',\n",
    "                   mincnt=.001\n",
    "                  )\n",
    "    cb = fig.colorbar(hb, ax=ax)\n",
    "    cb.set_label('log10(counts)', fontsize=8)\n",
    "\n",
    "    ax.plot(np.arange(.01, 100000),\n",
    "            np.power(10, poly(np.log10(np.arange(.01, 100000))))\n",
    "           )\n",
    "\n",
    "    ax.set_ylabel(\"Reads in Peaks\", fontsize=8)\n",
    "    ax.set_xlabel(\"TPM\", fontsize=8)\n",
    "    ax.set_xlim(.01, 100000)\n",
    "    ax.set_ylim(.01, 100000)\n",
    "    ax.set_yscale(\"log\", basey=10)\n",
    "    ax.set_xscale(\"log\", basex=10)\n",
    "    ax.set_title(\"RBFOX2 Expression Correlation\", fontsize=8)\n",
    "    \n",
    "    [tick.set_fontsize(8) for tick in ax.get_xticklabels()]\n",
    "    [tick.set_fontsize(8) for tick in ax.get_yticklabels()]\n",
    "    sns.despine(ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print scipy.stats.linregress(best_peaks_and_expression.normalized_read_count,\n",
    "                        best_peaks_and_expression.TPM)\n",
    "\n",
    "print scipy.stats.linregress(np.log10(best_peaks_and_expression.normalized_read_count), \n",
    "                             best_peaks_and_expression.TPM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we won't detect lowly expressed genes, but if you sequence a bit deeper you'll start picking that stuff up quickly, and it will stay consistant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I want to figure out a way to show change in detection between downsampled and non-downsampled datasets, maybe I'll go dive into the litature, althought this is somewhat of a unique question.  I think I'm honestly happy with my plot / regression.  The downsampling is unnessessary to solve this question, or at least its overkill.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_full_from_annotated(fn):\n",
    "    stripped_fn = \".\".join(fn.split(\".\")[:-3])\n",
    "    return stripped_fn + \".full.compressed2.bed.full\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Some Nans because a few genes don't have any gene expression at all.  Those get filtered out for the quartile\n",
    "#calculations up above\n",
    "\n",
    "def get_fraction_expressed(group, fraction=.1):\n",
    "    \"\"\"Returns the gene expression in each group, gets just that fractional expression, to describe new peaks detected\"\"\"\n",
    "    group = group.sort_values()\n",
    "    #return group.mean()\n",
    "    return group.iloc[int(len(group) * fraction)]\n",
    "\n",
    "def merge_peak_and_read_counts(df):\n",
    "        \n",
    "    df = get_recovered_peaks(df)\n",
    "        \n",
    "    df['stop'] = df.stop.astype(int)\n",
    "    df['normalized_read_count'] = df.ip_reads / (df.stop - df.start)\n",
    "\n",
    "    #Because I'm plotting on a log plot, if the TPM is 0 I can't use it.  So I set the TPM to the next lowest expression level\n",
    "    min_expression = df[df.TPM != 0].TPM.min()\n",
    "    df.ix[df.TPM == 0, \"TPM\"] = min_expression\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "r_values = defaultdict(dict)\n",
    "for name, df in tqdm(list(merged_data.groupby(level=[\"uID\", \"RBP\", \"Cell line\", \"rep\"]))):\n",
    "    cell_type = name[2]\n",
    "    \n",
    "    peaks_and_expression = merge_peak_and_read_counts(df).dropna()\n",
    "    \n",
    "    best_peaks_and_expression = peaks_and_expression.sort_values(\"normalized_read_count\", ascending=False)\n",
    "    best_peaks_and_expression = best_peaks_and_expression.groupby(\"gene_id\").first()\n",
    "    \n",
    "    bottom_10_percent = peaks_and_expression.groupby('fraction').TPM.agg(get_fraction_expressed)\n",
    "\n",
    "    r_values[name] = {\"r-value\": scipy.stats.linregress(np.log10(peaks_and_expression.normalized_read_count),\n",
    "                                                                   np.log10(peaks_and_expression.TPM)).rvalue,\n",
    "                      \"r-value_best\": scipy.stats.linregress(np.log10(best_peaks_and_expression.normalized_read_count),\n",
    "                                                             np.log10(best_peaks_and_expression.TPM)).rvalue,\n",
    "                                 'mean': bottom_10_percent.mean(),\n",
    "                                 'var': bottom_10_percent.var()}\n",
    "    \n",
    "r_values = pd.DataFrame(r_values)\n",
    "r_values = r_values.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "r2_values = np.power(r_values['r-value'], 2)\n",
    "print r2_values.mean()\n",
    "\n",
    "r2_values_to_share = r2_values.unstack()\n",
    "r2_values_to_share.to_csv(\"/projects/ps-yeolab3/encode/analysis/website_figures/peak_and_gene_expression_correlation.tsv\", sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_rows = 1\n",
    "num_cols = 1\n",
    "\n",
    "with dataviz.Figure(os.path.join(img_dir, \"rpkm_read_count_correlations_presentation.svg\"), figsize=(3* num_rows, 3*num_cols)) as fig:\n",
    "    ax = fig.add_subplot(num_cols,num_rows,1)\n",
    "    sns.violinplot(r2_values, cut=True, linewidth=.5, ax=ax)\n",
    "    sns.despine(ax=ax)\n",
    "    ax.set_xlabel(\"R2\", fontsize=18)\n",
    "    ax.set_ylabel(\"All Experiments\", fontsize=18)\n",
    "    ax.set_title(\"Correlation between reads\\nper peak and TPM of gene\", fontsize=18)\n",
    "    ax.set_xlim(0, 1)\n",
    "    [tick.set_fontsize(18) for tick in ax.get_xticklabels()]\n",
    "    [tick.set_fontsize(18) for tick in ax.get_yticklabels()]\n",
    "\n",
    "with dataviz.Figure(os.path.join(img_dir, \"rpkm_read_count_correlations.svg\"), figsize=(2.5* num_rows, 2.5*num_cols)) as fig:\n",
    "    ax = fig.add_subplot(num_cols,num_rows,1)\n",
    "    sns.violinplot(r2_values, cut=True, linewidth=.5, ax=ax)\n",
    "    sns.despine(ax=ax)\n",
    "    ax.set_xlabel(\"R2\", fontsize=8)\n",
    "    ax.set_ylabel(\"All Experiments\", fontsize=8)\n",
    "    ax.set_title(\"Correlation between reads\\nper peak and TPM of gene\", fontsize=8)\n",
    "    ax.set_xlim(0, 1)\n",
    "    [tick.set_fontsize(8) for tick in ax.get_xticklabels()]\n",
    "    [tick.set_fontsize(8) for tick in ax.get_yticklabels()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "r2_values.sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can explain that high correlation because its coating RBPs.  If an RBP coats is likely to have more correlated peak and gene expression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "r2_best_values = np.power(r_values['r-value_best'], 2)\n",
    "print r2_best_values.mean()\n",
    "\n",
    "r2_best_values_to_share = r2_best_values.unstack()\n",
    "r2_best_values_to_share.to_csv(\"/projects/ps-yeolab3/encode/analysis/website_figures/peak_and_gene_expression_correlation_best.tsv\", sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_rows = 1\n",
    "num_cols = 1\n",
    "with dataviz.Figure(os.path.join(img_dir, \"rpkm_read_count_correlations.svg\"), figsize=(4* num_rows, 4*num_cols)) as fig:\n",
    "    ax = fig.add_subplot(num_cols,num_rows,1)\n",
    "    sns.violinplot(r2_best_values, cut=True)\n",
    "    sns.despine(ax=ax)\n",
    "    ax.set_xlabel(\"$R^2$ Value\")\n",
    "    ax.set_ylabel(\"All Experiments\")\n",
    "    ax.set_title(\"Correlation between reads\\nper peak in gene and TPM of gene\")\n",
    "    ax.set_xlim(0, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# At what sequencing depth can you first detect a peak?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "results = []\n",
    "for name, df in tqdm(list(merged_data.groupby(level=[\"uID\", \"RBP\", \"Cell line\"]))): \n",
    "    uID, rbp, cell_line = name\n",
    "    \n",
    "    grp = df.groupby(level=\"rep\")\n",
    "    results.append(get_recovered_peaks(grp.get_group(\"rep1\")))\n",
    "    results.append(get_recovered_peaks(grp.get_group(\"rep2\")))\n",
    "results = pd.concat(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "first_detected = results.sort_values(\"reads\").groupby(\"gene_id\").first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(first_detected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "first_detected = first_detected[first_detected.TPM < 20000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_rows = 1\n",
    "num_cols = 1\n",
    "with dataviz.Figure(os.path.join(img_dir, \"reads_when_first_detected.svg\"), figsize=(4* num_rows, 4*num_cols)) as fig:\n",
    "    ax = fig.add_subplot(num_cols,num_rows,1)\n",
    "    ax.scatter(first_detected.TPM, first_detected.reads, alpha=\".7\", color=\".7\")\n",
    "    sns.despine(ax=ax)\n",
    "    ax.set_xlabel(\"TPM\")\n",
    "    ax.set_ylabel(\"Total Reads in Dataset\")\n",
    "    ax.set_xlim(.01, 10000)\n",
    "    ax.set_ylim(10000, 16000000)\n",
    "    ax.set_yscale(\"log\", basey=10)\n",
    "    ax.set_xscale(\"log\", basex=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mean_detected = results.sort_values(\"reads\").groupby(\"gene_id\").mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print first_detected[first_detected.TPM < .1].reads.min()\n",
    "print first_detected[first_detected.TPM < .1].reads.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print first_detected[first_detected.TPM < .1].reads.median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print first_detected[first_detected.TPM < .1].reads.mean()\n",
    "print first_detected[first_detected.TPM < .1].reads.var()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "1 * np.power(10, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_rows = 1\n",
    "num_cols = 1\n",
    "with dataviz.Figure(os.path.join(img_dir, \"reads_when_mean_detected_hexbin_presentation.svg\"), figsize=(5.0* num_cols, 4.* num_rows)) as fig:\n",
    "    ax = fig.add_subplot(num_cols,num_rows,1)\n",
    "    hb = ax.hexbin(first_detected.TPM, first_detected.reads, gridsize=25, \n",
    "                   bins='log',\n",
    "                   cmap='Greys',\n",
    "                   xscale='log',\n",
    "                   yscale='log',\n",
    "                   mincnt=.01)\n",
    "    ax.set_title(\"Number of reads in dataset\\nrequired to detect peak in gene\", fontsize=18)\n",
    "    cb = fig.colorbar(hb, ax=ax)\n",
    "    cb.set_label('log10(counts)', fontsize=18)\n",
    "    ax.set_xlim(.01, 10000)\n",
    "    ax.set_ylim(10000, 16000000)\n",
    "    ax.set_xlabel(\"TPM\", fontsize=18)\n",
    "    ax.set_ylabel(\"Reads in Dataset\", fontsize=18)\n",
    "    [tick.set_fontsize(16) for tick in ax.get_xticklabels()]\n",
    "    [tick.set_fontsize(16) for tick in ax.get_yticklabels()]\n",
    "\n",
    "    sns.despine(ax=ax)\n",
    "    \n",
    "with dataviz.Figure(os.path.join(img_dir, \"reads_when_mean_detected_hexbin_paper.svg\"), figsize=(2.5* num_rows, 2.5*num_cols)) as fig:\n",
    "    ax = fig.add_subplot(num_cols,num_rows,1)\n",
    "    hb = ax.hexbin(first_detected.TPM, first_detected.reads, gridsize=25, \n",
    "                   bins='log',\n",
    "                   cmap='Greys',\n",
    "                   xscale='log',\n",
    "                   yscale='log',\n",
    "                   mincnt=.01)\n",
    "    ax.set_title(\"Reads to detect peak\", fontsize=8)\n",
    "    cb = fig.colorbar(hb, ax=ax)\n",
    "    cb.set_label('log10(counts)', fontsize=8)\n",
    "    ax.set_xlim(.01, 10000)\n",
    "    ax.set_ylim(10000, 16000000)\n",
    "    ax.set_xlabel(\"TPM\", fontsize=8)\n",
    "    ax.set_ylabel(\"Reads\", fontsize=8)\n",
    "    [tick.set_fontsize(6) for tick in ax.get_xticklabels()]\n",
    "    [tick.set_fontsize(6) for tick in ax.get_yticklabels()]\n",
    "\n",
    "    sns.despine(ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "linregress = scipy.stats.linregress(np.log10(first_detected.TPM), np.log10(first_detected.reads))\n",
    "linregress"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So that fits nicely, and gives me a good model.  If you want to detect reads at say..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "poly = np.poly1d([linregress.slope, linregress.intercept])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.power(10, poly(np.log10(.01)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_rows = 1\n",
    "num_cols = 1\n",
    "with dataviz.Figure(os.path.join(img_dir, \"reads_when_mean_detected.svg\"), figsize=(4* num_rows, 4*num_cols)) as fig:\n",
    "    ax = fig.add_subplot(num_cols,num_rows,1)\n",
    "    ax.scatter(mean_detected.TPM, mean_detected.reads, alpha=\".7\", color=\".7\")\n",
    "    sns.despine(ax=ax)\n",
    "    ax.set_xlabel(\"TPM\")\n",
    "    ax.set_ylabel(\"Total Reads in Dataset\")\n",
    "    ax.set_xlim(.01, 10000)\n",
    "    ax.set_ylim(10000, 16000000)\n",
    "    ax.set_yscale(\"log\", basey=10)\n",
    "    ax.set_xscale(\"log\", basex=10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hmm... I don't know what to do here, I could dive deeper, and figure out the number of reads I need for a given gene "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lets look at de-novo motif identification, and then fraction of motifs detected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# results = []\n",
    "# for peak in peaks:\n",
    "#     results.append(format_clip_analysis(peak.fn, \"hg19_HepG2\", \"/projects/ps-yeolab3/encode/analysis/encode_v12/204_01_RBFOX2.merged.r2.bam\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# job_name = \"denovo_motif_analysis\"\n",
    "# job = qtools.Submitter(commands=results, \n",
    "#                  job_name=\"{}\".format(job_name), \n",
    "#                  sh_filename=\"/home/gpratt/projects/idr/scripts/{}.sh\".format(job_name),\n",
    "#                 array=True,\n",
    "#                 walltime=\"2:00:00\",\n",
    "#                 out_filename=\"/home/gpratt/projects/idr/scripts/{}.out\".format(job_name),\n",
    "#                 err_filename=\"/home/gpratt/projects/idr/scripts/{}.err\".format(job_name),\n",
    "#                 queue=\"home-yeo\")\n",
    "# job.job()\n",
    "\n",
    "# print \"/home/gpratt/projects/idr/scripts/{}.sh\".format(job_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The denovo results are running, now to just check the actual motif presence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "GCATG_hg19_motif = pybedtools.BedTool(\"/projects/ps-yeolab3/oolite_backup/gpratt/projects/pipeline_analysis/motifs/hg19/motif_GCATG.BED\").sort().saveas()\n",
    "merged_data['num_peaks'] = merged_data.appearing_peaks.progress_apply(lambda x: len(pybedtools.BedTool(x)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def calculate_motif_recovery(df, motif):\n",
    "    df = df.copy()\n",
    "    df['overlap'] = df.appearing_peaks.apply(lambda x: len(pybedtools.BedTool(x).intersect(motif, \n",
    "                                                                                           u=True, \n",
    "                                                                                           s=True, sorted=True)))\n",
    "    df['fraction_overlap_motif'] = df.overlap / df.num_peaks\n",
    "    \n",
    "    df['total_peaks'] = df.num_peaks.cumsum()\n",
    "    df['total_overlap'] = df.overlap.cumsum()\n",
    "    df['fraction_total_overlap_motif'] = df.total_overlap / df.total_peaks\n",
    "    return df\n",
    "\n",
    "def kmer_calc(fn, kmer):\n",
    "    fasta_name = fn + \".fasta\"\n",
    "    peak = pybedtools.BedTool(fn)\n",
    "    \n",
    "    fasta_peaks = peak.sequence(fi=\"/projects/ps-yeolab/genomes/hg19/chromosomes/all.fa\", fo=fasta_name, s=True)\n",
    "    result = []\n",
    "    for read in SeqIO.parse(fasta_name, \"fasta\"):\n",
    "        result.append(kmer in read.upper())\n",
    "    return sum(result)\n",
    "\n",
    "def calculate_motif_recovery_kmer(df, kmer):\n",
    "    real_kmer_calc = functools.partial(kmer_calc, kmer=kmer)\n",
    "    df['overlap_kmer'] = df.appearing_peaks.apply(real_kmer_calc)\n",
    "    df['fraction_overlap_kmer'] = df.overlap_kmer / df.num_peaks\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rep1 = merged_data.ix[('204', \"RBFOX2\", 'HepG2', 'rep1')]\n",
    "rep2 = merged_data.ix[('204', \"RBFOX2\", 'HepG2', 'rep2')]\n",
    "\n",
    "motif_recovery_rep1 = calculate_motif_recovery(rep1, GCATG_hg19_motif)\n",
    "motif_recovery_rep2 = calculate_motif_recovery(rep2, GCATG_hg19_motif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_cols = 1\n",
    "num_rows = 2\n",
    "\n",
    "x = np.concatenate([motif_recovery_rep1.index.values, motif_recovery_rep2.index.values])\n",
    "y = np.concatenate([motif_recovery_rep1.fraction_overlap_motif, motif_recovery_rep2.fraction_overlap_motif])\n",
    "\n",
    "print scipy.stats.linregress(x,y)\n",
    "\n",
    "with dataviz.Figure(os.path.join(img_dir, \"fraction_of_ugcaug_recovered.svg\"), figsize=(2.5* num_rows, 2.5*num_cols)) as fig:\n",
    "    ax = fig.add_subplot(num_cols,num_rows,1)\n",
    "    ax.plot(motif_recovery_rep1.index, motif_recovery_rep1.fraction_overlap_motif, linewidth=3, alpha=.7, label=\"Rep 1\")\n",
    "    ax.plot(motif_recovery_rep2.index, motif_recovery_rep2.fraction_overlap_motif, linewidth=3, alpha=.7, label=\"Rep 2\")\n",
    "\n",
    "    ax.set_ylabel(\"Fraction containing motif\", fontsize=8)\n",
    "    ax.set_xlabel(\"Fraction Downsampled\", fontsize=8)\n",
    "    ax.set_title(\"Fraction of newly recovered\\npeaks with GCUAG motif\", fontsize=8)\n",
    "    ax.set_ylim(0,.50)\n",
    "    [tick.set_fontsize(8) for tick in ax.get_xticklabels()]\n",
    "    [tick.set_fontsize(8) for tick in ax.get_yticklabels()]\n",
    "\n",
    "    sns.despine(ax=ax)\n",
    "    ax.legend(fontsize=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_cols = 1\n",
    "num_rows = 2\n",
    "\n",
    "x = np.concatenate([motif_recovery_rep1.index.values, motif_recovery_rep2.index.values])\n",
    "y = np.concatenate([motif_recovery_rep1.fraction_total_overlap_motif, motif_recovery_rep2.fraction_total_overlap_motif])\n",
    "\n",
    "print scipy.stats.linregress(x,y)\n",
    "\n",
    "with dataviz.Figure(os.path.join(img_dir, \"fraction_of_ugcaug_recovered_cumsum.svg\"), figsize=(2.5* num_rows, 2.5*num_cols)) as fig:\n",
    "    ax = fig.add_subplot(num_cols,num_rows,1)\n",
    "    ax.plot(motif_recovery_rep1.index, motif_recovery_rep1.fraction_total_overlap_motif, linewidth=2, alpha=.7, label=\"Rep 1\")\n",
    "    ax.plot(motif_recovery_rep2.index, motif_recovery_rep2.fraction_total_overlap_motif, linewidth=2, alpha=.7, label=\"Rep 2\")\n",
    "\n",
    "    ax.set_ylabel(\"Fraction containing motif\", fontsize=8)\n",
    "    ax.set_xlabel(\"Fraction Downsampled\", fontsize=8)\n",
    "    ax.set_title(\"Fraction RBFOX2 peaks with GCUAG motif\", fontsize=8)\n",
    "    ax.set_ylim(0,.50)\n",
    "    [tick.set_fontsize(8) for tick in ax.get_xticklabels()]\n",
    "    [tick.set_fontsize(8) for tick in ax.get_yticklabels()]\n",
    "\n",
    "    sns.despine(ax=ax)\n",
    "    ax.legend(loc=0, fontsize=8)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "motif_recovery_rep1_kmer = calculate_motif_recovery_kmer(rep1, \"TGCATG\")\n",
    "motif_recovery_rep2_kmer = calculate_motif_recovery_kmer(rep2, \"TGCATG\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_cols = 1\n",
    "num_rows = 2\n",
    "with dataviz.Figure(os.path.join(img_dir, \"fraction_of_ugcaug_recovered.svg\"), figsize=(4* num_rows, 4*num_cols)) as fig:\n",
    "    ax = fig.add_subplot(num_cols,num_rows,1)\n",
    "    ax.plot(motif_recovery_rep1_kmer.index, motif_recovery_rep1_kmer.fraction_overlap_kmer, linewidth=3, alpha=.7, label=\"Rep 1\")\n",
    "    ax.plot(motif_recovery_rep2_kmer.index, motif_recovery_rep2_kmer.fraction_overlap_kmer, linewidth=3, alpha=.7, label=\"Rep 2\")\n",
    "\n",
    "    ax.set_ylabel(\"Fraction of new recovered Peaks with GCUAG Motif\")\n",
    "    ax.set_xlabel(\"Fraction Downsampled\")\n",
    "    ax.set_title(\"Quality of Recovered RBFOX2 Peaks\")\n",
    "    ax.set_ylim(0,.50)\n",
    "    sns.despine(ax=ax)\n",
    "    ax.legend()\n",
    "    [tick.set_fontsize(8) for tick in ax.get_xticklabels()]\n",
    "    [tick.set_fontsize(8) for tick in ax.get_yticklabels()]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Look at the splicing factors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "five_prime_ends = pybedtools.BedTool(\"/home/gpratt/clipper/clipper/data/regions/hg19_five_prime_ends.bed\")\n",
    "three_prime_ends = pybedtools.BedTool(\"/home/gpratt/clipper/clipper/data/regions/hg19_three_prime_ends.bed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "colors = sns.color_palette(\"Set2\", 2)\n",
    "cell_types = [\"HepG2\", \"K562\"]\n",
    "cell_type_colors = dict(zip(cell_types, colors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rep1 = merged_data.ix[('204', \"RBFOX2\", 'HepG2', 'rep1')]\n",
    "rep2 = merged_data.ix[('204', \"RBFOX2\", 'HepG2', 'rep2')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def plot_both_cell_types(rbp, bedtool, site_name, total=False):\n",
    "    num_cols = 1\n",
    "    num_rows = 1\n",
    "    \n",
    "    if total:\n",
    "        plot_y = \"fraction_total_overlap_motif\"\n",
    "    else:\n",
    "        plot_y = \"fraction_overlap_motif\"\n",
    "    out_type = os.path.splitext(os.path.basename(bedtool.fn))[0]\n",
    "    cell_types = merged_data.xs(rbp, level=\"RBP\")\n",
    "    \n",
    "    total_flag = \"total\" if total else \"fractional\"\n",
    "    x = np.array([])\n",
    "    y = np.array([])\n",
    "    with dataviz.Figure(os.path.join(img_dir, \"fraction_of_{}_{}_{}_recovered.svg\".format(rbp, out_type, total_flag)), figsize=(2.5* num_rows, 2.5*num_cols)) as fig:\n",
    "        ax = fig.add_subplot(num_cols,num_rows,1)\n",
    "        for name, group in cell_types.groupby(level=[\"uID\", \"Cell line\", 'rep']):\n",
    "            cell_type = name[1]\n",
    "            motif_recovery_rep1 = calculate_motif_recovery(group, bedtool)\n",
    "            ax.plot(motif_recovery_rep1.index.get_level_values(level=\"fraction\"), \n",
    "                    motif_recovery_rep1[plot_y], \n",
    "                    linewidth=3, \n",
    "                    alpha=.7, \n",
    "                    label=\" \".join(name),\n",
    "                    color=cell_type_colors[cell_type]\n",
    "                   )\n",
    "            \n",
    "            x = np.concatenate([x, motif_recovery_rep1.index.get_level_values(level=\"fraction\").values])\n",
    "            y = np.concatenate([y, motif_recovery_rep1[plot_y]])\n",
    "        ax.set_ylabel(\"Fraction of recovered\\npeaks overlapping with {}\".format(site_name), fontsize=8)\n",
    "        ax.set_xlabel(\"Peaks recovered at fraction\", fontsize=8)\n",
    "        ax.set_title(\"Fractions of peaks overlapping with {}\".format(site_name), fontsize=8)\n",
    "        ax.set_ylim(0,1)\n",
    "        sns.despine(ax=ax)\n",
    "        ax.legend(fontsize=8)\n",
    "        [tick.set_fontsize(8) for tick in ax.get_xticklabels()]\n",
    "        [tick.set_fontsize(8) for tick in ax.get_yticklabels()]\n",
    "\n",
    "    return scipy.stats.linregress(x, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "u2af1_3_ss_regressor = plot_both_cell_types(\"U2AF1\", five_prime_ends, \"3' splice site\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "u2af1_3_ss_regressor_total = plot_both_cell_types(\"U2AF1\", five_prime_ends, \"3' splice site\", total=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "u2af1_3_ss_regressor = plot_both_cell_types(\"U2AF1\", five_prime_ends, \"3' splice site\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "u2af1_3_ss_regressor_total = plot_both_cell_types(\"U2AF1\", five_prime_ends, \"3' splice site\", total=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "u2af1_5_ss_regressor = plot_both_cell_types(\"U2AF1\", three_prime_ends, \"5' splice site\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "u2af1_5_ss_regressor_total = plot_both_cell_types(\"U2AF1\", three_prime_ends, \"5' splice site\", total=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "u2af2_3_ss_regressor = plot_both_cell_types(\"U2AF2\", five_prime_ends, \"3' splice site\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "u2af2_3_ss_regressor_total = plot_both_cell_types(\"U2AF2\", five_prime_ends, \"3' splice site\", total=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "u2af2_5_ss_regressor = plot_both_cell_types(\"U2AF2\", three_prime_ends, \"5' splice site\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "u2af2_5_ss_regressor_total = plot_both_cell_types(\"U2AF2\", three_prime_ends, \"5' splice site\", total=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "prpf8_5_ss_regressor = plot_both_cell_types(\"PRPF8\", three_prime_ends, \"5' splice site\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "prpf8_5_ss_regressor_total = plot_both_cell_types(\"PRPF8\", three_prime_ends, \"5' splice site\", total=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rmbm22_5_ss_regressor = plot_both_cell_types(\"RBM22\", three_prime_ends, \"5' splice site\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rmbm22_5_ss_regressor_total = plot_both_cell_types(\"RBM22\", three_prime_ends, \"5' splice site\", total=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rmbm22_3_ss_regressor = plot_both_cell_types(\"RBM22\", five_prime_ends, \"3' splice site\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rmbm22_3_ss_regressor_total = plot_both_cell_types(\"RBM22\", five_prime_ends, \"3' splice site\", total=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sf3b4_5_ss_regressor = plot_both_cell_types(\"SF3B4\", three_prime_ends, \"5' splice site\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sf3b4_5_ss_regressor_total = plot_both_cell_types(\"SF3B4\", three_prime_ends, \"5' splice site\", total=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sf3b4_3_ss_regressor = plot_both_cell_types(\"SF3B4\", five_prime_ends, \"3' splice site\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sf3b4_3_ss_regressor_total = plot_both_cell_types(\"SF3B4\", five_prime_ends, \"3' splice site\", total=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sf3b1_5_ss_regressor = plot_both_cell_types(\"SF3B1\", three_prime_ends, \"5' splice site\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sf3b1_5_ss_regressor_total = plot_both_cell_types(\"SF3B1\", three_prime_ends, \"5' splice site\", total=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sf3b1_3_ss_regressor = plot_both_cell_types(\"SF3B1\", five_prime_ends, \"3' splice site\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sf3b1_3_ss_regressor_total = plot_both_cell_types(\"SF3B1\", five_prime_ends, \"3' splice site\", total=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sf3b1_3_ss_regressor = plot_both_cell_types(\"SF3B1\", five_prime_ends, \"3' splice site\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rbfox2_gcaug_regressor = plot_both_cell_types(\"RBFOX2\", GCATG_hg19_motif, \"UGCAUG Motifs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rbfox2_gcaug_regressor_total = plot_both_cell_types(\"RBFOX2\", GCATG_hg19_motif, \"UGCAUG Motifs\", total=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rbfox2_gcaug_regressor_total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To validate we looked at RBPs known to bind the splicesome, both 3' and 5' splice sites and looked at the fracton of peaks overlapping those regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_both_cell_types_presentation(rbp, bedtool, site_name, total=False):\n",
    "    num_cols = 1\n",
    "    num_rows = 1\n",
    "    \n",
    "    if total:\n",
    "        plot_y = \"fraction_total_overlap_motif\"\n",
    "    else:\n",
    "        plot_y = \"fraction_overlap_motif\"\n",
    "    out_type = os.path.splitext(os.path.basename(bedtool.fn))[0]\n",
    "    cell_types = merged_data.xs(rbp, level=\"RBP\")\n",
    "    \n",
    "    total_flag = \"total\" if total else \"fractional\"\n",
    "    x = np.array([])\n",
    "    y = np.array([])\n",
    "    with dataviz.Figure(os.path.join(img_dir, \"fraction_of_{}_{}_{}_recovered_presentation.svg\".format(rbp, out_type, total_flag)), figsize=(5* num_rows, 5*num_cols)) as fig:\n",
    "        ax = fig.add_subplot(num_cols,num_rows,1)\n",
    "        for name, group in cell_types.groupby(level=[\"uID\", \"Cell line\", 'rep']):\n",
    "            cell_type = name[1]\n",
    "            motif_recovery_rep1 = calculate_motif_recovery(group, bedtool)\n",
    "            ax.plot(motif_recovery_rep1.index.get_level_values(level=\"fraction\"), \n",
    "                    motif_recovery_rep1[plot_y], \n",
    "                    linewidth=3, \n",
    "                    alpha=.7, \n",
    "                    label=\" \".join(name),\n",
    "                    color=cell_type_colors[cell_type]\n",
    "                   )\n",
    "            \n",
    "            x = np.concatenate([x, motif_recovery_rep1.index.get_level_values(level=\"fraction\").values])\n",
    "            y = np.concatenate([y, motif_recovery_rep1[plot_y]])\n",
    "        ax.set_ylabel(\"Fraction of peaks\\nwith {}\".format(site_name), fontsize=20)\n",
    "        ax.set_xlabel(\"Fraction Downsampled\", fontsize=20)\n",
    "        ax.set_title(\"Fractions of peaks with {}\".format(site_name), fontsize=20)\n",
    "        ax.set_xticks(np.arange(0,1.1, .2))\n",
    "        ax.set_ylim(0,1)\n",
    "        sns.despine(ax=ax)\n",
    "        ax.legend(fontsize=20)\n",
    "        [tick.set_fontsize(20) for tick in ax.get_xticklabels()]\n",
    "        [tick.set_fontsize(20) for tick in ax.get_yticklabels()]\n",
    "\n",
    "    return scipy.stats.linregress(x, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rbfox2_gcaug_regressor_total = plot_both_cell_types_presentation(\"RBFOX2\", GCATG_hg19_motif, \"UGCAUG Motif\", total=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "u2af2_3_ss_regressor_total = plot_both_cell_types_presentation(\"U2AF2\", five_prime_ends, \"3' splice site\", total=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_both_cell_types_test(rbp, bedtool, site_name, total=False):\n",
    "    num_cols = 1\n",
    "    num_rows = 1\n",
    "    \n",
    "    if total:\n",
    "        plot_y = \"fraction_total_overlap_motif\"\n",
    "    else:\n",
    "        plot_y = \"fraction_overlap_motif\"\n",
    "    out_type = os.path.splitext(os.path.basename(bedtool.fn))[0]\n",
    "    cell_types = merged_data.xs(rbp, level=\"RBP\")\n",
    "    \n",
    "    total_flag = \"total\" if total else \"fractional\"\n",
    "    x = np.array([])\n",
    "    y = np.array([])\n",
    "    with dataviz.Figure(os.path.join(img_dir, \"fraction_of_{}_{}_{}_recovered.svg\".format(rbp, out_type, total_flag)), figsize=(2.5* num_rows, 2.5*num_cols)) as fig:\n",
    "        ax = fig.add_subplot(num_cols,num_rows,1)\n",
    "        for name, group in cell_types.groupby(level=[\"uID\", \"Cell line\", 'rep']):\n",
    "            cell_type = name[1]\n",
    "            motif_recovery_rep1 = calculate_motif_recovery(group, bedtool)\n",
    "            motif_recovery_rep1['percent_decrease'] = (motif_recovery_rep1[plot_y] - motif_recovery_rep1[plot_y][-1]) / motif_recovery_rep1[plot_y][-1]\n",
    "            #return motif_recovery_rep1\n",
    "            ax.plot(motif_recovery_rep1.index.get_level_values(level=\"fraction\"), \n",
    "                    motif_recovery_rep1['percent_decrease'], \n",
    "                    linewidth=3, \n",
    "                    alpha=.7, \n",
    "                    label=\" \".join(name),\n",
    "                    color=cell_type_colors[cell_type]\n",
    "                   )\n",
    "            \n",
    "            x = np.concatenate([x, motif_recovery_rep1.index.get_level_values(level=\"fraction\").values])\n",
    "            y = np.concatenate([y, motif_recovery_rep1[plot_y]])\n",
    "        ax.set_ylabel(\"Fraction of recovered\\npeaks overlapping with {}\".format(site_name), fontsize=8)\n",
    "        ax.set_xlabel(\"Peaks recovered at fraction\", fontsize=8)\n",
    "        ax.set_title(\"Fractions of peaks overlapping with {}\".format(site_name), fontsize=8)\n",
    "        #ax.set_ylim(0,1)\n",
    "        sns.despine(ax=ax)\n",
    "        ax.legend(fontsize=8)\n",
    "        [tick.set_fontsize(8) for tick in ax.get_xticklabels()]\n",
    "        [tick.set_fontsize(8) for tick in ax.get_yticklabels()]\n",
    "\n",
    "    return scipy.stats.linregress(x, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rbfox2_gcaug_regressor_total = plot_both_cell_types_test(\"RBFOX2\", GCATG_hg19_motif, \"UGCAUG Motifs\", total=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rbfox2_gcaug_regressor_total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Look at conservation\n",
    "\n",
    "loading old pickle files won't work because I collapse them too much"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "conservation_out = \"/projects/ps-yeolab3/encode/analysis/conservation_peaks\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "conservation = {}\n",
    "grp = merged_data.groupby(level=[\"uID\", \"RBP\", \"Cell line\", 'rep'])\n",
    "for name, row in tqdm(list(total_bedtools.iterrows())):\n",
    "    bedtool = pybedtools.BedTool(row.filtered_moderate)\n",
    "    \n",
    "    out_file = os.path.splitext(bedtool.fn)[0] + \".tab\"\n",
    "    if os.path.exists(out_file):\n",
    "        result = pd.read_table(out_file, \n",
    "                               index_col=0,\n",
    "                               header=None, \n",
    "                               names=['name', 'size', 'covered', 'sum', 'mean0', 'mean'])\n",
    "    else:\n",
    "        result = CLIP_analysis.bigWigAverageOverBed(\"/projects/ps-yeolab/genomes/hg19/hg19_phastcons.bw\", bedtool)\n",
    "    peaks_and_read_counts = get_recovered_peaks(grp.get_group(name))\n",
    "    \n",
    "    chrom_lst = []\n",
    "    start_lst = []\n",
    "    stop_lst = []\n",
    "\n",
    "    for index in result.index:\n",
    "        index = index.split(\"_\")[-1]\n",
    "        chrom, loc = index.split(\":\")\n",
    "        start, stop = loc.split(\"-\")\n",
    "\n",
    "\n",
    "        chrom_lst.append(chrom)\n",
    "        start_lst.append(int(start))\n",
    "        stop_lst.append(int(stop))\n",
    "\n",
    "    result['chrom'] = chrom_lst\n",
    "    result['start'] = start_lst\n",
    "    result['stop'] = stop_lst\n",
    "    \n",
    "    peaks_and_conservation = pd.merge(peaks_and_read_counts, result, \n",
    "             left_on=[\"chrom\", \"start\", \"stop\"],\n",
    "             right_on=[\"chrom\", \"start\", \"stop\"],\n",
    "             how=\"inner\")\n",
    "    \n",
    "    out_name, ext = os.path.splitext(row.filtered_moderate)\n",
    "    out_name = os.path.basename(out_name)\n",
    "    out_name = os.path.join(conservation_out, out_name + \".conservation\" + ext)\n",
    "    \n",
    "    peaks_and_conservation.to_csv(out_name)\n",
    "    if len(peaks_and_read_counts) != len(peaks_and_conservation):\n",
    "        print out_name\n",
    "    \n",
    "    peak_conservation = peaks_and_conservation.groupby(\"fraction\")['mean']\n",
    "    conservation[name] = pd.concat({'fractional_mean': peak_conservation.mean(),\n",
    "                                    \"cumsum_mean\": peak_conservation.sum().cumsum() / peak_conservation.count().cumsum()},\n",
    "                                   axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "conservation = pd.concat(conservation)\n",
    "merged_data['fractional_conserved_mean'] = conservation['fractional_mean']\n",
    "merged_data['cumsum_conserved_mean'] = conservation['cumsum_mean']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "grp = merged_data.groupby(level=[\"uID\", \"RBP\", \"Cell line\", \"rep\"])\n",
    "df_rep1 = grp.get_group(('204', 'RBFOX2', 'HepG2', \"rep1\"))\n",
    "df_rep2 = grp.get_group(('204', 'RBFOX2', 'HepG2', \"rep2\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_cols = 1\n",
    "num_rows = 1\n",
    "out_type = os.path.splitext(os.path.basename(bedtool.fn))[0]\n",
    "cell_types = merged_data.xs(rbp, level=\"RBP\")\n",
    "with dataviz.Figure(os.path.join(img_dir, \"rbfox2_conservation.svg\"), figsize=(4* num_rows, 4*num_cols)) as fig:\n",
    "    ax = fig.add_subplot(1,1,1)\n",
    "    ax.plot(df_rep1.index.get_level_values(\"fraction\"), df_rep1['fractional_conserved_mean'])\n",
    "    ax.plot(df_rep2.index.get_level_values(\"fraction\"), df_rep2['fractional_conserved_mean'])\n",
    "    ax.set_xlabel(\"Fraction Downsampled\")\n",
    "    ax.set_ylabel(\"Mean Conservation\")\n",
    "    sns.despine(ax=ax)\n",
    "    \n",
    "x = np.concatenate([df_rep1.index.values, df_rep2.index.values])\n",
    "y = np.concatenate([df_rep1.fractional_conserved_mean, df_rep2.fractional_conserved_mean])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_cols = 1\n",
    "num_rows = 1\n",
    "out_type = os.path.splitext(os.path.basename(bedtool.fn))[0]\n",
    "cell_types = merged_data.xs(rbp, level=\"RBP\")\n",
    "with dataviz.Figure(os.path.join(img_dir, \"rbfox2_conservation_cumsum.svg\"), figsize=(2.5* num_rows, 2.5*num_cols)) as fig:\n",
    "    ax = fig.add_subplot(1,1,1)\n",
    "    ax.plot(df_rep1.index.get_level_values(\"fraction\"), df_rep1['cumsum_conserved_mean'], label=\"Rep 1\")\n",
    "    ax.plot(df_rep2.index.get_level_values(\"fraction\"), df_rep2['cumsum_conserved_mean'], label=\"Rep 2\")\n",
    "    ax.set_xlabel(\"Fraction Downsampled\", fontsize=8)\n",
    "    ax.set_ylabel(\"Mean Conservation\", fontsize=8)\n",
    "    ax.set_title(\"Conservation of RBFOX2 Peaks\", fontsize=8)\n",
    "    sns.despine(ax=ax)\n",
    "    ax.legend(fontsize=8)\n",
    "    [tick.set_fontsize(8) for tick in ax.get_xticklabels()]\n",
    "    [tick.set_fontsize(8) for tick in ax.get_yticklabels()]\n",
    "\n",
    "x = np.concatenate([df_rep1.index.get_level_values(level=\"fraction\"), df_rep2.index.get_level_values(level=\"fraction\")])\n",
    "y = np.concatenate([df_rep1.cumsum_conserved_mean, df_rep2.cumsum_conserved_mean])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "linregress = scipy.stats.linregress(x,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I don't really know what I was doing here\n",
    "\n",
    "Looks like I'm trying to get all the datasets with a significant relationship between conservation and sequencing depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "grp = merged_data.groupby(level=[\"uID\", \"RBP\", \"Cell line\"])\n",
    "result = {}\n",
    "for name, df in grp:\n",
    "    x = np.array([])\n",
    "    y = np.array([])\n",
    "    for rep_name, rep_df in df.groupby(level=\"rep\"):\n",
    "        x = np.concatenate([x, rep_df.index.get_level_values(level=\"fraction\").values])\n",
    "        y = np.concatenate([y, rep_df.fractional_conserved_mean])   \n",
    "    \n",
    "    linregress = scipy.stats.linregress(x,y)\n",
    "    result[name] = {\"slope\": linregress.slope,\n",
    "                   \"intercept\": linregress.intercept,\n",
    "                   \"rvalue\": linregress.rvalue,\n",
    "                   'pvalue': linregress.pvalue,\n",
    "                   'stderr': linregress.stderr,}\n",
    "result = pd.DataFrame(result).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "result_sig = result[result.pvalue < (.05 / 181)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "result_sig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_cols = 1\n",
    "num_rows = 1\n",
    "out_type = os.path.splitext(os.path.basename(bedtool.fn))[0]\n",
    "cell_types = merged_data.xs(rbp, level=\"RBP\")\n",
    "with dataviz.Figure(os.path.join(img_dir, \"rbfox2_conservation.svg\"), figsize=(4* num_rows, 4*num_cols)) as fig:\n",
    "    ax = fig.add_subplot(1,1,1)\n",
    "    ax.plot(df_rep1.index.get_level_values(\"fraction\"), df_rep1['fractional_conserved_mean'])\n",
    "    ax.plot(df_rep2.index.get_level_values(\"fraction\"), df_rep2['fractional_conserved_mean'])\n",
    "    ax.set_xlabel(\"Fraction Downsampled\")\n",
    "    ax.set_ylabel(\"Mean Conservation\")\n",
    "    sns.despine(ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_both_cell_types_conservation(rbp):\n",
    "    num_cols = 1\n",
    "    num_rows = 1\n",
    "    out_type = os.path.splitext(os.path.basename(bedtool.fn))[0]\n",
    "    cell_types = merged_data.xs(rbp, level=\"RBP\")\n",
    "    \n",
    "    x = np.array([])\n",
    "    y = np.array([])\n",
    "    with dataviz.Figure(os.path.join(img_dir, \"{}_conservation_presentation.svg\".format(rbp)), figsize=(4* num_rows, 4*num_cols)) as fig:\n",
    "        ax = fig.add_subplot(num_cols,num_rows,1)\n",
    "        for name, group in cell_types.groupby(level=[\"uID\", \"Cell line\", 'rep']):\n",
    "            cell_type = name[1]\n",
    "            ax.plot(group.index.get_level_values(level=\"fraction\"), \n",
    "                    group['cumsum_conserved_mean'], \n",
    "                    linewidth=3, \n",
    "                    alpha=.7, \n",
    "                    label=\" \".join(name),\n",
    "                    color=cell_type_colors[cell_type]\n",
    "                   )\n",
    "            \n",
    "            x = np.concatenate([x, group.index.get_level_values(level=\"fraction\").values])\n",
    "            y = np.concatenate([y, group.cumsum_conserved_mean])\n",
    "        ax.set_ylabel(\"Mean Conservation\".format(), fontsize=18)\n",
    "        ax.set_xlabel(\"Fraction Downsampled\", fontsize=18)\n",
    "        ax.set_title(\"Conservation of Peaks\", fontsize=18)\n",
    "        ax.set_ylim(0,1)\n",
    "        sns.despine(ax=ax)\n",
    "        ax.legend(fontsize=18)\n",
    "        ax.set_xticks(np.arange(0,1.1, .2))\n",
    "        [tick.set_fontsize(18) for tick in ax.get_xticklabels()]\n",
    "        [tick.set_fontsize(18) for tick in ax.get_yticklabels()]\n",
    "\n",
    "    return scipy.stats.linregress(x, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_both_cell_types_conservation(\"RBFOX2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_both_cell_types_conservation(\"U2AF2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I don't really know what I'm doing here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Plot without any filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fraction_bound_full(row):\n",
    "    total_bedtool = total_bedtools.ix[row.name[:-1]].input_norm\n",
    "    fractional_bedtool = row.input_norm\n",
    "    \n",
    "    total_bedtool = pybedtools.BedTool(total_bedtool)\n",
    "    fractional_bedtool = pybedtools.BedTool(fractional_bedtool)\n",
    "    return len(total_bedtool.intersect(fractional_bedtool, u=True, s=True, sorted=True)) / float(len(total_bedtool))\n",
    " \n",
    "def fraction_of_fraction_in_total_full(row):\n",
    "    total_bedtool = total_bedtools.ix[row.name[:-1]].input_norm\n",
    "    fractional_bedtool = row.input_norm\n",
    "\n",
    "    total_bedtool = pybedtools.BedTool(total_bedtool)\n",
    "    fractional_bedtool = pybedtools.BedTool(fractional_bedtool)\n",
    "    return len(fractional_bedtool.intersect(total_bedtool, u=True, s=True, sorted=True)) / float(len(fractional_bedtool))\n",
    "\n",
    "def fraction_not_in_total_full(row):\n",
    "    total_bedtool = total_bedtools.ix[row.name[:-1]].input_norm\n",
    "    fractional_bedtool = row.input_norm\n",
    "\n",
    "    total_bedtool = pybedtools.BedTool(total_bedtool)\n",
    "    fractional_bedtool = pybedtools.BedTool(fractional_bedtool)\n",
    "    return len(fractional_bedtool.intersect(total_bedtool, v=True, s=True, sorted=True)) / float(len(fractional_bedtool))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "merged_data['fraction_overlap_full'] = merged_data.apply(fraction_bound_full, axis=1)\n",
    "merged_data['fraction_not_in_total_full'] = merged_data.apply(fraction_not_in_total_full, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "total_datasets = len(result_df.groupby(level=0))\n",
    "num_cols = 4\n",
    "num_rows = (total_datasets / 4) + 1\n",
    "count = 0\n",
    "\n",
    "with dataviz.Figure(os.path.join(\"/home/gpratt/Dropbox/encode_integration/qc_work/\", \"downsampling_peaks.svg\"), figsize=(4* num_cols, 4*num_rows)) as fig:\n",
    "    for uID, df in result_df.groupby(level=0):\n",
    "        count += 1\n",
    "        ax = fig.add_subplot(num_rows, num_cols, count)\n",
    "\n",
    "        for rep, dataset_df in df.groupby(level=1):\n",
    "            name = \"{} {}\".format(uID, fully_downsampled_data[fully_downsampled_data.uID == uID].RBP.values[0])\n",
    "            ax.plot(dataset_df[reads], dataset_df.fraction_of_peaks, \n",
    "                    label=name, \n",
    "                    alpha=.7, \n",
    "                    linewidth=3)\n",
    "            ax.xaxis.set_major_formatter(xfmt)\n",
    "            ax.set_xlabel(\"Reads (M)\")\n",
    "            ax.set_ylabel(\"Fraction Overlapped\")\n",
    "            ax.set_ylim(0,1)\n",
    "            sns.despine(ax=ax)\n",
    "        ax.legend(fontsize=18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "total_datasets = len(fraction_not_in_total_unfiltered_df.groupby(level=0))\n",
    "num_cols = 4\n",
    "num_rows = (total_datasets / 4) + 1\n",
    "count = 0\n",
    "\n",
    "with dataviz.Figure(os.path.join(\"/home/gpratt/Dropbox/encode_integration/qc_work/\", \"downsampling_peaks.svg\"), figsize=(4* num_cols, 4*num_rows)) as fig:\n",
    "    for uID, df in fraction_not_in_total_unfiltered_df.groupby(level=0):\n",
    "        count += 1\n",
    "        ax = fig.add_subplot(num_rows, num_cols, count)\n",
    "\n",
    "        for rep, dataset_df in df.groupby(level=1):\n",
    "            name = \"{} {}\".format(uID, fully_downsampled_data[fully_downsampled_data.uID == uID].RBP.values[0])\n",
    "            ax.plot(dataset_df[reads], dataset_df.fraction_of_peaks, \n",
    "                    label=name, \n",
    "                    alpha=.7, \n",
    "                    linewidth=3)\n",
    "            ax.xaxis.set_major_formatter(xfmt)\n",
    "            ax.set_xlabel(\"Reads (M)\")\n",
    "            ax.set_ylabel(\"Fraction Overlapped\")\n",
    "            ax.set_ylim(0,1)\n",
    "            sns.despine(ax=ax)\n",
    "        ax.legend(fontsize=18)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# So the unfiltered peaks appear to overlap nicely.  Why do they go away? I'll look at RBFOX2 as a case study.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I want peaks that are siginifant in 09, but not significant in full, but exist in full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print unfiltered_full_fn\n",
    "print unfiltered_downsampled_fn\n",
    "print get_full_from_annotated(unfiltered_full_fn)\n",
    "print get_dowmsample_from_annotated(unfiltered_downsampled_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "unfiltered_full_fn = fully_downsampled_data.ix[1]['uID_01.basedon_uID_01.peaks.l2inputnormnew.bed.compressed.bed']\n",
    "unfiltered_downsampled_fn = fully_downsampled_data.ix[1]['CLIP_rep1_09']\n",
    "\n",
    "unfiltered_full_peaks = pybedtools.BedTool(unfiltered_full_fn)\n",
    "\n",
    "filtered_full_peaks = pybedtools.BedTool(make_and_filter_clipper_moderate(unfiltered_full_fn))\n",
    "filtered_downsampled_peaks = pybedtools.BedTool(make_and_filter_clipper_moderate(unfiltered_downsampled_fn))\n",
    "\n",
    "not_in_full_dataset = filtered_downsampled_peaks.intersect(filtered_full_peaks, v=True, s=True)\n",
    "peaks_filtered_from_full = not_in_full_dataset.intersect(unfiltered_full_peaks, s=True, wo=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "peaks_filtered_from_full_df = peaks_filtered_from_full.to_dataframe(names=[\"chrom_subset\", \"start_subset\", \n",
    "                                      \"stop_subset\", \"name_subset\", \n",
    "                                      \"score_subset\", \"strand_subset\",\n",
    "                                      \"chrom_full\", \"start_full\", \n",
    "                                      \"stop_full\", \"name_full\", \n",
    "                                      \"score_full\", \"strand_full\", \"annotation_full\",\n",
    "                                      \"gene_full\", \"foo_full\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_dowmsample_from_annotated(fn):\n",
    "    stripped_fn = \".\".join(fn.split(\".\")[:-2])\n",
    "    return stripped_fn + \".bed.full\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print get_dowmsample_from_annotated(unfiltered_downsampled_fn)\n",
    "print os.path.exists(get_dowmsample_from_annotated(unfiltered_downsampled_fn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "full_counts = pd.read_table(get_full_from_annotated(unfiltered_full_fn), \n",
    "                           names=[\"chrom\", \"start\", \"stop\", \"full_name\", \"ip_reads\", \"input_reads\", \"p_val\", \"fold_change\", \"something\", \"enrichment\", \"log10_p_val\", \"log2_fold_change\"])\n",
    "\n",
    "downsampled_counts = pd.read_table(get_dowmsample_from_annotated(unfiltered_downsampled_fn), \n",
    "                           names=[\"chrom\", \"start\", \"stop\", \"full_name\", \"ip_reads\", \"input_reads\", \"p_val\", \"fold_change\", \"something\", \"enrichment\", \"log10_p_val\", \"log2_fold_change\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "merged_with_full = pd.merge(peaks_filtered_from_full_df, full_counts, \n",
    "                            left_on=[\"chrom_full\", \"start_full\", \"stop_full\"], \n",
    "                            right_on=[\"chrom\", \"start\", \"stop\"], \n",
    "                            how=\"left\",\n",
    "                            suffixes=[\"bedtool\", \"full\"])\n",
    "\n",
    "merged_full_downsampled = pd.merge(merged_with_full, downsampled_counts,\n",
    "                                   left_on=[\"chrom_subset\", \"start_subset\", \"stop_subset\"], \n",
    "                                   right_on=[\"chrom\", \"start\", \"stop\"], \n",
    "                                   how=\"left\",\n",
    "                                   suffixes=['_full_count', '_downsampled_count'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "subset_length = merged_full_downsampled.stop_subset - merged_full_downsampled.start_subset\n",
    "full_length = merged_full_downsampled.stop_full - merged_full_downsampled.start_full\n",
    "\n",
    "num_rows = 2\n",
    "num_cols = 3\n",
    "with dataviz.Figure(os.path.join(\"/home/gpratt/Dropbox/encode_integration/qc_work/\", \"peak_downsampling_figure.svg\"), figsize=(5* num_cols, 5*num_rows)) as fig:\n",
    "        ax = fig.add_subplot(num_rows, num_cols, 1)\n",
    "        ax.scatter(subset_length, full_length)\n",
    "        ax.set_xlabel(\"Subset Peak Length (bp)\")\n",
    "        ax.set_ylabel(\"Full Peak Length (bp)\")\n",
    "        ax.set_title(\"Overlapping Peak Lengths\")\n",
    "        sns.despine(ax=ax)\n",
    "\n",
    "        ax = fig.add_subplot(num_rows, num_cols, 2)\n",
    "        ax.scatter(merged_full_downsampled.log10_p_val_downsampled_count, merged_full_downsampled.log10_p_val_full_count)\n",
    "        ax.set_xlabel(\"Downsample p-value\")\n",
    "        ax.set_ylabel(\"Full p-value\")\n",
    "        ax.set_title(\"Log 10 P-value\")\n",
    "        ax.axvline(3)\n",
    "        ax.axhline(3)\n",
    "\n",
    "        sns.despine(ax=ax)\n",
    "        \n",
    "        ax = fig.add_subplot(num_rows, num_cols, 3)\n",
    "        ax.scatter(merged_full_downsampled.log2_fold_change_downsampled_count, merged_full_downsampled.log2_fold_change_full_count)\n",
    "        ax.set_xlabel(\"Downsample Fold Change\")\n",
    "        ax.set_ylabel(\"Full Fold Change\")\n",
    "        ax.set_title(\"Fold Changes\")\n",
    "        ax.axvline(3)\n",
    "        ax.axhline(3)\n",
    "        sns.despine(ax=ax)\n",
    "        \n",
    "        ax = fig.add_subplot(num_rows, num_cols, 4)\n",
    "        ax.scatter(np.log10(merged_full_downsampled.ip_reads_downsampled_count), \n",
    "                   np.log10(merged_full_downsampled.ip_reads_full_count))\n",
    "        ax.set_xlabel(\"Downsample  IP Read Count\")\n",
    "        ax.set_ylabel(\"Full IP Read Count\")\n",
    "        ax.set_title(\"IP Read Count\")\n",
    "        ax.axvline(3)\n",
    "        ax.axhline(3)\n",
    "        sns.despine(ax=ax)\n",
    "        \n",
    "        ax = fig.add_subplot(num_rows, num_cols, 5)\n",
    "        ax.scatter(np.log10(merged_full_downsampled.input_reads_downsampled_count), \n",
    "                   np.log10(merged_full_downsampled.input_reads_full_count))\n",
    "        ax.set_xlabel(\"Downsample Input Read Count\")\n",
    "        ax.set_ylabel(\"Full Input Read Count\")\n",
    "        ax.set_title(\"Input Read Count\")\n",
    "        ax.axvline(3)\n",
    "        ax.axhline(3)\n",
    "        sns.despine(ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(merged_full_downsampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(merged_full_downsampled[merged_full_downsampled.ip_reads_downsampled_count > merged_full_downsampled.ip_reads_full_count])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "scipy.stats.linregress(subset_length, full_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Conclusion from here is that the peak lengths don't really change too much.  R2 isn't great, but its alright?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_rows = 1\n",
    "num_cols = 1\n",
    "with dataviz.Figure(os.path.join(\"/home/gpratt/Dropbox/encode_integration/qc_work/\", \"peak_downsampling_figure.svg\"), figsize=(5* num_cols, 5*num_rows)) as fig:\n",
    "        ax = fig.add_subplot(num_rows, num_cols, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "total_datasets = len(result_df.groupby(level=0))\n",
    "num_rows = 1\n",
    "num_cols = 1\n",
    "count = 0\n",
    "\n",
    "with dataviz.Figure(os.path.join(\"/home/gpratt/Dropbox/encode_integration/qc_work/\", \"peak_downsampling_figure.svg\"), figsize=(5* num_cols, 5*num_rows)) as fig:\n",
    "        count += 1\n",
    "        ax = fig.add_subplot(num_rows, num_cols, count)\n",
    "\n",
    "#         for rep, dataset_df in foo.get_group(\"244\").groupby(level=1):\n",
    "#             ax.plot(dataset_df[reads], dataset_df.fraction_of_peaks, label=fully_downsampled_data[fully_downsampled_data.uID == \"244\"].RBP.values[0], color='b')\n",
    "        uid = \"228\"\n",
    "        for rep, dataset_df in foo.get_group(uid).groupby(level=1):\n",
    "            ax.plot(dataset_df[reads], dataset_df.fraction_of_peaks, label=fully_downsampled_data[fully_downsampled_data.uID == uid].RBP.values[0],\n",
    "                    alpha=.7, linewidth=3, color='r')\n",
    "            break\n",
    "\n",
    "        uid = \"204\"\n",
    "        for rep, dataset_df in foo.get_group(uid).groupby(level=1):\n",
    "            ax.plot(dataset_df.reads, dataset_df.fraction_of_peaks, label=fully_downsampled_data[fully_downsampled_data.uID == uid].RBP.values[0], \n",
    "                    alpha=.7, linewidth=3, color='b')\n",
    "            break\n",
    "            \n",
    "#         for rep, dataset_df in foo.get_group(\"262\").groupby(level=1):\n",
    "#             ax.plot(dataset_df[reads], dataset_df.fraction_of_peaks, label=fully_downsampled_data[fully_downsampled_data.uID == \"262\"].RBP.values[0], \n",
    "#                     alpha=.7, linewidth=3, color='y')\n",
    "#             break\n",
    "\n",
    "        ax.set_xlabel(\"Number of reads\", fontsize=18)\n",
    "        ax.set_ylabel(\"Fraction of peaks overlapping all peaks\", fontsize=18)\n",
    "        [tick.set_fontsize(14) for tick in ax.get_xticklabels()]\n",
    "        [tick.set_fontsize(14) for tick in ax.get_yticklabels()]\n",
    "        sns.despine(ax=ax)\n",
    "        ax.legend()\n",
    "        ax.get_xaxis().set_major_formatter(matplotlib.ticker.FuncFormatter(lambda x, p: format(int(x), ',')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "total_datasets = len(result_df.groupby(level=0))\n",
    "num_rows = 1\n",
    "num_cols = 1\n",
    "count = 0\n",
    "\n",
    "with dataviz.Figure(os.path.join(\"/home/gpratt/Dropbox/encode_integration/qc_work/\", \"peak_downsampling_figure_slbp.svg\"), figsize=(5* num_cols, 5*num_rows)) as fig:\n",
    "        count += 1\n",
    "        ax = fig.add_subplot(num_rows, num_cols, count)\n",
    "       \n",
    "        for rep, dataset_df in foo.get_group(\"262\").groupby(level=1):\n",
    "            ax.plot(dataset_df[reads], dataset_df.fraction_of_peaks, label=fully_downsampled_data[fully_downsampled_data.uID == \"262\"].RBP.values[0], \n",
    "                    alpha=.7, linewidth=3, color='y')\n",
    "            break\n",
    "\n",
    "#         for rep, dataset_df in foo.get_group(\"243\").groupby(level=1):\n",
    "#             ax.plot(dataset_df[reads], dataset_df.fraction_of_peaks, label=fully_downsampled_data[fully_downsampled_data.uID == \"243\"].RBP.values[0], \n",
    "#                     alpha=.7, linewidth=3, color='g')\n",
    "#             break\n",
    "\n",
    "        ax.set_xlabel(\"Number of reads\", fontsize=18)\n",
    "        ax.set_ylabel(\"Fraction of peaks overlapping all peaks\", fontsize=18)\n",
    "        ax.set_ylim(0,1)\n",
    "        [tick.set_fontsize(8) for tick in ax.get_xticklabels()]\n",
    "        [tick.set_fontsize(8) for tick in ax.get_yticklabels()]\n",
    "        sns.despine(ax=ax)\n",
    "        ax.legend()\n",
    "        ax.get_xaxis().set_major_formatter(matplotlib.ticker.FuncFormatter(lambda x, p: format(int(x), ',')))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Try more accurate model of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
