{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline \n",
    "\n",
    "from collections import defaultdict\n",
    "import datetime\n",
    "import glob \n",
    "import math\n",
    "import itertools\n",
    "import os \n",
    "\n",
    "import numpy as np\n",
    "import pysam\n",
    "import scipy\n",
    "import seaborn as sns\n",
    "from matplotlib import gridspec\n",
    "import matplotlib\n",
    "import pandas as pd\n",
    "from IPython.core.display import HTML\n",
    "import sklearn\n",
    "\n",
    "from gscripts import qtools\n",
    "from gscripts.general import dataviz\n",
    "from gscripts.encode import encode_helpers\n",
    "\n",
    "sns.set_style(\"darkgrid\")\n",
    "\n",
    "img_dir = \"/home/gpratt/Dropbox/Pratt_Gabriel/PapersInProgress/eCLIP_qc/working_figures/fig_2\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make proper bam file again\n",
    "(make bam files great again)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "foo\n"
     ]
    }
   ],
   "source": [
    "print \"foo\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fixed_bam_out_dir = \"/home/gpratt/projects/idr/analysis/fixed_bam_out_dir\"\n",
    "def fixed_bam_name(fn):\n",
    "    out_fn = os.path.basename(fn)\n",
    "    out_fn = \".\".join(out_fn.split(\".\")[:4] + ['rmRep', 'bam'])\n",
    "    out_file = os.path.join(fixed_bam_out_dir, out_fn)\n",
    "    return out_file\n",
    "\n",
    "def fix_eric_sam(fn):\n",
    "    out_file = fixed_bam_name(fn)\n",
    "    return \"python ~/gscripts/gscripts/clipseq/fix_sam_file.py --sam {} | samtools view -bS - > {}\".format(fn, out_file)\n",
    "\n",
    "def format_rmrep(fn, shuffle=False):\n",
    "    base_fn = os.path.basename(os.path.splitext(fn)[0])\n",
    "    base_fn = os.path.join(downsample_path, base_fn)\n",
    "    if shuffle:\n",
    "        shuffle_fn = fn + \".shuffled\"\n",
    "        shuffle_code = \"samtools bamshuf {0} {1}\".format(fn, shuffle_fn)\n",
    "        #change input to the shuffled, ordered file\n",
    "        fn = shuffle_fn + \".bam\"\n",
    "    rm_rep_fn = base_fn + \".rmRep.bam\"\n",
    "    metrics_fn = base_fn + \".rmRep.metrics\"\n",
    "    out_fn = base_fn + \".rmRep.sorted.bam\"\n",
    "    collapse_code = \"barcode_collapse_pe.py -b {input_fn} -o {rm_rep} -m {metrics} && samtools sort {rm_rep} -o {out_fn} && samtools index {out_fn}\".format(input_fn=fn, rm_rep=rm_rep_fn, metrics=metrics_fn, out_fn=out_fn)\n",
    "    if shuffle:\n",
    "        return \" && \".join([shuffle_code, collapse_code])\n",
    "    else:\n",
    "        return collapse_code\n",
    "    \n",
    "def format_rmrep_out_name(fn):\n",
    "    base_fn = os.path.splitext(fn)[0]\n",
    "    return base_fn + \".rmRep.sorted.bam\"\n",
    "\n",
    "def format_sort(fn):\n",
    "    base_fn = os.path.splitext(fn)[0]\n",
    "    out_fn = base_fn + \".sorted.bam\"\n",
    "    return \"samtools sort {input_fn} -o {out_fn} && samtools index {out_fn}\".format(input_fn=fn, out_fn=out_fn)\n",
    "\n",
    "def sorted_file_name(fn):\n",
    "    base_file = os.path.basename(fn)\n",
    "    stripped_base = os.path.splitext(base_file)[0]\n",
    "    \n",
    "    out_file = os.path.join(downsample_path, stripped_base + \".sorted.bam\")\n",
    "    return out_file\n",
    "\n",
    "def sort_and_index(fn):\n",
    "    out_file = sorted_file_name(fn)\n",
    "    intermedeate_file = out_file\n",
    "\n",
    "    return \"samtools sort {0} -o {1} && samtools index {2}\".format(fn, intermedeate_file, out_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "strange_sams = glob.glob(\"/home/elvannostrand/scratch/ENCODE_20170429_newannotations_FINAL/*.mapped_vs_MASTER_filelist.wrepbaseandtRNA.fa.fixed.fa.UpdatedSimpleRepeat.sam.combined_w_uniquemap.prermDup.sam.gz\")\n",
    "strange_sams += glob.glob(\"/home/elvannostrand/scratch/ENCODE_20170429_newannotations_FINAL/*.combined_w_uniquemap.rmDup.sam.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sams_to_fix = [fix_eric_sam(fn) for fn in strange_sams if not os.path.exists(fixed_bam_name(fn))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_slices = 500\n",
    "full_process_list = []\n",
    "for x in range(num_slices):\n",
    "    cur_list = sams_to_fix[x::num_slices]\n",
    "    if len(cur_list) > 0:\n",
    "        full_process_list.append(\" && \".join(cur_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#571_merged are files we generated, but don't actually use.  Ignoring for now, just using regular 571 files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/gpratt/projects/idr/scripts/make_proper_bams.sh\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "running 0 tasks as an array-job.\n"
     ]
    }
   ],
   "source": [
    "job_name = \"make_proper_bams\"\n",
    "job = qtools.Submitter(commands=full_process_list, \n",
    "                 job_name=\"{}\".format(job_name), \n",
    "                 sh_filename=\"/home/gpratt/projects/idr/scripts/{}.sh\".format(job_name),\n",
    "                array=True,\n",
    "                walltime=\"6:00:00\",\n",
    "                out_filename=\"/home/gpratt/projects/idr/scripts/{}.out\".format(job_name),\n",
    "                err_filename=\"/home/gpratt/projects/idr/scripts/{}.err\".format(job_name),\n",
    "                queue=\"home-yeo\")\n",
    "job.job()\n",
    "\n",
    "print \"/home/gpratt/projects/idr/scripts/{}.sh\".format(job_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "collapsed_bams = glob.glob(\"/home/elvannostrand/scratch/ENCODE_20170429_newannotations_FINAL/*.combined_w_uniquemap.rmDup.sam.gz\")\n",
    "sams_to_fix = [format_sort(fixed_bam_name(fn)) for fn in collapsed_bams if os.path.exists(fixed_bam_name(fn))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "running 500 tasks as an array-job.\n",
      "job ID: 8864890\n",
      "running 500 tasks as an array-job.\n",
      "job ID: 8864891\n",
      "running 143 tasks as an array-job.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/gpratt/projects/idr/scripts/sort_family_map_rmdup.sh\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "job ID: 8864892\n"
     ]
    }
   ],
   "source": [
    "job_name = \"sort_family_map_rmdup\"\n",
    "job = qtools.Submitter(commands=sams_to_fix, \n",
    "                 job_name=\"{}\".format(job_name), \n",
    "                 sh_filename=\"/home/gpratt/projects/idr/scripts/{}.sh\".format(job_name),\n",
    "                array=True,\n",
    "                walltime=\"6:00:00\",\n",
    "                out_filename=\"/home/gpratt/projects/idr/scripts/{}.out\".format(job_name),\n",
    "                err_filename=\"/home/gpratt/projects/idr/scripts/{}.err\".format(job_name),\n",
    "                queue=\"home-yeo\")\n",
    "job.job()\n",
    "\n",
    "print \"/home/gpratt/projects/idr/scripts/{}.sh\".format(job_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scale to all encode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#original path\n",
    "#downsample_path = \"/home/gpratt/projects/idr/analysis/downsample_unshuffled/\"\n",
    "downsample_path = \"/home/gpratt/projects/idr/analysis/downsample_unshuffled_v2/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "merged_data = encode_helpers.get_merged_data()\n",
    "#Filter out anything sequenced after 16/7/18\n",
    "merged_data[merged_data['Submitted Date'] < datetime.date(2016, 7, 18)]\n",
    "#Filter out anything not_qced\n",
    "merged_data = merged_data[merged_data.is_qced]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "merged_data['file_id'] = merged_data.CLIP.apply(lambda x: \"_\".join(os.path.basename(x).split(\"_\")[:-1]))\n",
    "merged_data_reindex = merged_data.reset_index()\n",
    "merged_data_reindex['short_rep'] = merged_data_reindex.rep.apply(lambda x: \"01\" if x == \"rep1\" else \"02\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ip_file_list_foo = pd.read_csv(\"/home/gpratt/Dropbox/encode_integration/for_eric/full_IP_file_list.csv\", index_col=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ip_file_list = pd.read_csv(\"/home/gpratt/Dropbox/encode_integration/for_eric/full_IP_file_list.csv\", index_col=0)\n",
    "\n",
    "#File names that don't match with master file, fixing here\n",
    "ip_file_list = ip_file_list.replace(\"TRNC6A\", \"TNRC6A\")\n",
    "ip_file_list = ip_file_list.replace(\"BOP1\", \"NOLC1\")\n",
    "ip_file_list = ip_file_list.replace(\"EIF2B\", \"EIF2S2\")\n",
    "ip_file_list = ip_file_list.replace(\"RO60-TROVE2\", \"TROVE2\")\n",
    "ip_file_list = ip_file_list.replace(\"KHDRBS1-SAM68\", \"KHDRBS1\")\n",
    "ip_file_list = ip_file_list.replace(\"ADAR1\", \"ADAR\")\n",
    "ip_file_list = ip_file_list.replace(\"RDBP\", \"TARDBP\")\n",
    "ip_file_list = ip_file_list.replace(\"PKM2\", \"PKM\")\n",
    "\n",
    "#Only consider data that is in the final dataset\n",
    "bam_ids = merged_data.file_id.values\n",
    "\n",
    "ip_file_list = ip_file_list[['ENCODE_ID', 'RBP', 'processing_name_1', 'processing_name_2']]\n",
    "ip_file_list = ip_file_list[ip_file_list.ENCODE_ID.isin(bam_ids)]\n",
    "\n",
    "#The rule is always match the final datasets format, or a tidy version of that format, sometimes file names don't match \n",
    "#actual uID 203_01 is actually 271_01, we fix that here\n",
    "#Need to convert ENCODE_ID to uID + rep number\n",
    "result = []\n",
    "for name, row in ip_file_list.iterrows():\n",
    "    new_id = merged_data_reindex[merged_data_reindex.file_id == row.ENCODE_ID].iloc[0]\n",
    "    result.append(\"_\".join([new_id.uID, new_id.short_rep]))\n",
    "    \n",
    "ip_file_list['ENCODE_ID'] = result\n",
    "\n",
    "ip_file_list = ip_file_list.set_index([\"ENCODE_ID\", \"RBP\"])\n",
    "ip_file_list.columns = pd.MultiIndex.from_tuples(((\"processing_name\", 1.0, \"rep1\"), (\"processing_name\", 1.0, \"rep2\")), \n",
    "                                                 names=['CLIP', \"fraction\", \"rep\"])\n",
    "\n",
    "ip_file_list = ip_file_list.stack().stack()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "these should all be equal, problem if otherwise\n",
      "654 654 654.0\n"
     ]
    }
   ],
   "source": [
    "print \"these should all be equal, problem if otherwise\"\n",
    "print len(merged_data), len(bam_ids), len(ip_file_list) / 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def make_single_base(fn):\n",
    "    r1_name = fn.split(\";\")[0]\n",
    "    base_fn = os.path.basename(r1_name)\n",
    "    return \".\".join(base_fn.split(\".\")[:2])\n",
    "\n",
    "#original path\n",
    "#master_path = \"/projects/ps-yeolab3/encode/analysis/encode_master/\"\n",
    "master_path = fixed_bam_out_dir\n",
    "\n",
    "def make_unshuffled_bam(basename):\n",
    "    return os.path.join(master_path, basename + \".adapterTrim.round2.rmRep.bam\")\n",
    "\n",
    "def make_rmdup_bam(basename):\n",
    "    #This is a typo I'll want to fix in the full run\n",
    "    return os.path.join(downsample_path, basename + \".adapterTrim.round2.rmRep.rmRep.sorted.bam\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ip_file_list['basename'] = ip_file_list.processing_name.apply(make_single_base)\n",
    "ip_file_list['unshuffled_bam'] = ip_file_list.basename.apply(make_unshuffled_bam)\n",
    "ip_file_list['rmdup_bam'] = ip_file_list.basename.apply(make_rmdup_bam)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make all the Downsampled bams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ip_file_list = ip_file_list.unstack()\n",
    "for fraction in range(1,10):\n",
    "    bam_replacement = \".0{}.bam\".format(fraction)\n",
    "    ip_file_list[('unshuffled_bam', fraction / 10.)] = ip_file_list['unshuffled_bam', 1.0].apply(lambda x: downsample_path + os.path.basename(x).replace(\".bam\", bam_replacement))\n",
    "    \n",
    "ip_file_list = ip_file_list.stack()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "#run only files ready to be run\n",
    "origional_files = ip_file_list.xs(1.0, level=\"fraction\")\n",
    "ungenerated_files = origional_files[~origional_files.unshuffled_bam.apply(os.path.exists)]\n",
    "print len(ungenerated_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "unrun_ip_file_list = ip_file_list[~(ip_file_list.unshuffled_bam.apply(os.path.exists))]\n",
    "unrun_items = unrun_ip_file_list.groupby(level=[\"ENCODE_ID\", \"RBP\", \"rep\"]).groups.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Can't sort because then rumdup doesn't work anymore...\n",
    "\n",
    "def format_unshuffled_downsample(key):\n",
    "    unshuffled_bams = ip_file_list.loc[key].unshuffled_bam\n",
    "    #Makes a big assumption about the order of everything, this will probably come back to bite me later\n",
    "    rep = \"downsample_bam.py  --no_sort '--bam' '{}'  '--bam01' '{}' '--bam02' '{}'  '--bam03' '{}'  '--bam04' '{}' '--bam05' '{}'  '--bam06' '{}'  '--bam07' '{}'  '--bam08' '{}'  '--bam09' '{}'\".format(*unshuffled_bams.values)\n",
    "    return rep\n",
    "\n",
    "downsampled_list = []\n",
    "for key in unrun_items:\n",
    "    downsampled_list.append(format_unshuffled_downsample(key))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#These files don't exist because they haven't been processed yet\n",
    "len(downsampled_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_slices = 500\n",
    "full_process_list = []\n",
    "for x in range(num_slices):\n",
    "    cur_list = downsampled_list[x::num_slices]\n",
    "    if len(cur_list) > 0:\n",
    "        full_process_list.append(\" && \".join(cur_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(full_process_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "running 0 tasks as an array-job.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "job = qtools.Submitter(commands=full_process_list, \n",
    "                 job_name=\"downsample_unshuffled\", \n",
    "                 sh_filename=\"/home/gpratt/projects/idr/scripts/downsample_unshuffled.sh\",\n",
    "                array=True,\n",
    "                walltime=\"6:00:00\",\n",
    "                out_filename=\"/home/gpratt/projects/idr/scripts/downsample_unshuffled.out\",\n",
    "                err_filename=\"/home/gpratt/projects/idr/scripts/downsample_unshuffled.err\",\n",
    "                queue=\"home-yeo\")\n",
    "job.job()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove PCR Duplciates\n",
    "(Don't run until previous code is done running)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "null_ip_file_list = ip_file_list.rmdup_bam.isnull()\n",
    "ip_file_list.loc[null_ip_file_list, 'rmdup_bam'] = ip_file_list.loc[null_ip_file_list].unshuffled_bam.apply(format_rmrep_out_name)\n",
    "ip_file_list['sorted_bam'] = ip_file_list.unshuffled_bam.apply(sorted_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "remaining_jobs = []\n",
    "\n",
    "unsorted_files = ip_file_list[~ip_file_list.sorted_bam.apply(os.path.exists)]\n",
    "remaining_jobs += list(unsorted_files.unshuffled_bam.apply(sort_and_index).values)\n",
    "\n",
    "#This is a bit silly, but I need to seperate out the fully run file from the unrun ones\n",
    "import functools\n",
    "format_full_rmrep = functools.partial(format_rmrep, shuffle=True)\n",
    "\n",
    "unrmduped_files = ip_file_list[~ip_file_list[\"rmdup_bam\"].apply(os.path.exists)]\n",
    "remaining_jobs += list(unrmduped_files.unshuffled_bam.xs(1.0, level=\"fraction\").apply(format_full_rmrep).values)\n",
    "remaining_jobs += list(unrmduped_files[~unrmduped_files.index.get_level_values(level=\"fraction\").isin([1.0])].unshuffled_bam.apply(format_rmrep).values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(remaining_jobs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_slices = 500\n",
    "full_process_list = []\n",
    "for x in range(num_slices):\n",
    "    cur_list = remaining_jobs[x::num_slices]\n",
    "    if len(cur_list) > 0:\n",
    "        full_process_list.append(\" && \".join(cur_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(full_process_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "job = qtools.Submitter(commands=full_process_list, \n",
    "                 job_name=\"downsample_rmdup\", \n",
    "                 sh_filename=\"/home/gpratt/projects/idr/scripts/downsample_rmdup.sh\",\n",
    "                array=True,\n",
    "                walltime=\"8:00:00\",\n",
    "                out_filename=\"/home/gpratt/projects/idr/scripts/downsample_rmdup.out\",\n",
    "                err_filename=\"/home/gpratt/projects/idr/scripts/downsample_rmdup.err\",\n",
    "                       queue=\"home-yeo\")\n",
    "job.job()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try the adjectency graph approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "umi_out_dir = \"/home/gpratt/projects/idr/analysis/umi_downsampling\"\n",
    "\n",
    "def umi_name(fn):\n",
    "    out_file_base = os.path.join(umi_out_dir, os.path.splitext(os.path.basename(fn))[0])\n",
    "    out_file = out_file_base + \".dedup.bam\"\n",
    "    return out_file\n",
    "\n",
    "def sorted_umi_name(fn):\n",
    "    out_file = umi_name(fn)\n",
    "    sorted_file = os.path.splitext(out_file)[0] + \".sorted.bam\"\n",
    "    return sorted_file\n",
    "\n",
    "def umi_downsample(fn):\n",
    "    out_file = umi_name(fn)\n",
    "    sorted_file = sorted_umi_name(fn)\n",
    "    \n",
    "    out_file_base = os.path.join(umi_out_dir, os.path.splitext(os.path.basename(fn))[0])\n",
    "    \n",
    "    refrormatted_fn = out_file_base + \".adjusted.bam\"\n",
    "    metrics_name = out_file_base + \".dedup\"\n",
    "    reformat_command = \"python /home/gpratt/gscripts/gscripts/clipseq/adjust_umi_location.py --in_bam {} --out_bam {}\".format(fn, refrormatted_fn)\n",
    "    index_command = \"samtools index {}\".format(refrormatted_fn)\n",
    "    sort_command = \"samtools sort {} -o {} 2> /dev/null\".format(out_file, sorted_file)\n",
    "    index_command_2 = \"samtools index {}\".format(sorted_file)\n",
    "    #umi_command = \"umi_tools dedup -I {} --output-stats {} -S {}  --paired\".format(refrormatted_fn, metrics_name, out_file)\n",
    "    #Apparently the stats output slows things down, getting rid of it for now\n",
    "    umi_command = \"umi_tools dedup -I {}  -S {} --paired\".format(refrormatted_fn, out_file)\n",
    "\n",
    "    return \" && \".join([reformat_command, index_command, umi_command, sort_command, index_command_2])\n",
    "    #return \" && \".join([sort_command, index_command_2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ip_file_list['directional_bam'] = ip_file_list.sorted_bam.apply(sorted_umi_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "unrun_files = ip_file_list[~ip_file_list['directional_bam'].apply(os.path.exists)]\n",
    "remaining_jobs = list(unrun_files.sorted_bam.apply(umi_downsample).values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(remaining_jobs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a perfect world I'd re-run all of this downsampling stuff with my fixed bam splitter, some 01 datasets don't have any reads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_slices = 500\n",
    "full_process_list = []\n",
    "for x in range(num_slices):\n",
    "    cur_list = remaining_jobs[x::num_slices]\n",
    "    if len(cur_list) > 0:\n",
    "        full_process_list.append(\" && \".join(cur_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(full_process_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "out_root = \"adjecency_matrix_rmdup\"\n",
    "job = qtools.Submitter(commands=full_process_list, \n",
    "                 job_name=out_root, \n",
    "                 sh_filename=\"/home/gpratt/projects/idr/scripts/{}.sh\".format(out_root),\n",
    "                array=True,\n",
    "                walltime=\"48:00:00\",\n",
    "                out_filename=\"/home/gpratt/projects/idr/scripts/{}.out\".format(out_root),\n",
    "                err_filename=\"/home/gpratt/projects/idr/scripts/{}.err\".format(out_root),\n",
    "                       queue=\"home-yeo\")\n",
    "job.job()\n",
    "\n",
    "print \"/home/gpratt/projects/idr/scripts/{}.sh\".format(out_root)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Count and process all generated data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm not getting all data because the umi stuff is crashing on datasets with very low read numbers,  I'm calling this fine because we don't want that data anyway. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "run_ip_file_list = ip_file_list[['sorted_bam', 'directional_bam', 'rmdup_bam']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "run_ip_file_list.head().applymap(os.path.exists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "files_cant_process = run_ip_file_list.applymap(os.path.exists)\n",
    "files_cant_process[~files_cant_process.apply(all, axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "genomic_chroms = ['chr1', 'chr2', 'chr3', 'chr4', 'chr5', 'chr6', 'chr7', 'chr8', 'chr8', 'chr9',\n",
    "'chr10','chr11', 'chr12', 'chr13', 'chr14', 'chr15', 'chr16', 'chr17', 'chr18', 'chr18', 'chr19'\n",
    "'chr20','chr21', 'chr22', 'chr23', 'chrX', 'chrY', 'chrM']\n",
    "\n",
    "def get_chrom_mapped_reads(fn):\n",
    "    samtool = pysam.Samfile(fn)\n",
    "\n",
    "    count = 0\n",
    "    for read in samtool:\n",
    "        if read.reference_name in genomic_chroms:\n",
    "            count += 1\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#If I need to re-run this I'll want to switch to an idxstats approach, my current appraoch took like a week to re-process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%timeit\n",
    "get_chrom_mapped_reads(run_ip_file_list['sorted_bam'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%timeit\n",
    "pysam.idxstats(\"/home/gpratt/projects/idr/analysis/downsample_unshuffled_v2/Sample-202_S5_R1.C01_202_01_PTBP1.adapterTrim.round2.rmRep.sorted.bam\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "counts_list = counts_list.drop(\"directional_bam\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "counts_list = run_ip_file_list.copy()\n",
    "counts_list['sorted_bam'] = run_ip_file_list['sorted_bam'].apply(encode_helpers.get_mapped_reads)\n",
    "counts_list['sorted_bam_genome_unique'] = run_ip_file_list['sorted_bam'].apply(get_chrom_mapped_reads)\n",
    "\n",
    "counts_list['directional_bam'] = run_ip_file_list['directional_bam'].apply(encode_helpers.get_mapped_reads)\n",
    "counts_list['rmdup_bam'] = run_ip_file_list['rmdup_bam'].apply(encode_helpers.get_mapped_reads)\n",
    "counts_list['rmdup_bam_genome_unique'] = run_ip_file_list['rmdup_bam'].apply(get_chrom_mapped_reads)\n",
    "\n",
    "#Correct for the fact that I'm counting mapped reads, not mapped pairs here\n",
    "counts_list_adjusted = counts_list / 2\n",
    "\n",
    "counts_list_adjusted.to_csv(\"downsample_counts_v5.txt\")\n",
    "run_ip_file_list.to_csv(\"run_file_list_v5.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are some issues with duplicate IDs, below is a gchat from Eric detailing what version to use in this study (can ignore now that we have the master list)\n",
    "\n",
    "1. 235 submitted 4000\n",
    "1. 284 - correct 4000 run is labeled “284_01_4000_fixed_DDX21” cid 284x4000fix\n",
    "1. 285 submitted 4000\n",
    "1. 323 not submitted, use 4000\n",
    "1. 333 not submitted, use 4000\n",
    "1. 355 not submitted, use 4000\n",
    "1. 363 not submitted, use 4000\n",
    "1. 374 4000 is only rep1, use 2500\n",
    "1. 386 not submitted, use 4000\n",
    "1. 390 submitted 4000\n",
    "1. 403 not submitted, use 4000\n",
    "1. 451 not submitted, use 4000\n",
    "1. 485 not submitted, use 4000\n",
    "1. 486 not submitted, use 4000\n",
    "1. 632 - submitted 632x, labeled 632_01_bc2rev_SUB1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "new_index = []\n",
    "for ENCODE_ID, RBP, rep, fraction in counts_list_adjusted.index:\n",
    "    split_id = ENCODE_ID.split(\"_\")\n",
    "    uID = split_id[0]\n",
    "    bio_rep = \"_\".join(split_id[1:])\n",
    "    \n",
    "    new_index.append([uID, RBP, bio_rep, rep, fraction])\n",
    "    \n",
    "counts_list_final = counts_list_adjusted.copy()\n",
    "counts_list_final.index = pd.MultiIndex.from_tuples(new_index, \n",
    "                                                    names=[\"RBP_ID\", \"RBP\", \"bio_rep\", \"tech_rep\", \"fraction\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "counts_list_final = counts_list_final.reset_index()\n",
    "counts_list_final = counts_list_final.set_index(['RBP_ID', \"RBP\", \"bio_rep\", \"tech_rep\", \"fraction\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Uncollapsed: raw data\n",
    "#directional umi_tools dedup directional method\n",
    "#unique my unique collapsing method\n",
    "#counts_list_final.columns = ['uncollapsed', 'directional', 'unique']\n",
    "counts_list_final.columns = ['uncollapsed', 'unique', 'genomic_uncollapsed', 'genomic_unique']\n",
    "\n",
    "counts_list_final.to_csv(\"downsample_counts_full_v5.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
